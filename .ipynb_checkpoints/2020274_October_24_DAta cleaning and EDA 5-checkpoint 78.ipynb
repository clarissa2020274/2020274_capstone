{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f1b685",
   "metadata": {},
   "source": [
    "## MSc Data Analytics - Capstone Project\n",
    "\n",
    "#### Predictive Analysis in the Coffee Market: Using Deep Learning to predict coffee prices.\n",
    "Student id: 2020274 Clarissa Cardoso\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This notebook aims to analyse historical coffee price indices and develop a predictive model for future price trends in the green coffee market. The focus is on using data from the ICO (International Coffee Organization), particularly the composite prices indec (I-CIP), that combines prices of Colombian Milds, Other Milds, Brazilian Naturals, and Robustas.\n",
    "\n",
    "### Dataset:\n",
    "The dataset used in this analysis consists of historical coffee price data, with daily observations for business days. Prices are expressed in cents of USD per lb. and stated on a\n",
    "differential basis to the New York and London futures exchanges (https://icocoffee.org/wp-content/uploads/2023/01/icc-105-17-r1e-rules-indicator-prices.pdf)\n",
    "\n",
    "The data utilized in this project is sourced from the International Coffee Organization's (ICO) Public Market Information (https://ico.org/resources/public-market-information/), which provides the I-CIP values free of charge.\n",
    "\n",
    "For the early stages of the experimentation, 1 year worth of data was available to collect, from 01Feb23 to 29Feb24, which is present on a separate notebook (2020274_capstone_EDA_Models 2.ipynb). In this notebook, recent data from March to September 2024 were added to expand insights and feed more datapoints to modelling stage. \n",
    "\n",
    "\n",
    "### Objectives:\n",
    "1. Clean and preprocess the dataset for missing values and inconsistencies.\n",
    "2. Explore the time-series behavior of coffee prices through visualizations.\n",
    "3. Implement various forecasting models to predict future price trends, including traditional statistical models (e.g., ARIMA/Sarima) and deep learning algorithms (e.g., LSTM neural networks).\n",
    "4. Compare model performance using key metrics (e.g., RMSE, MAE).\n",
    "\n",
    "\n",
    "### Expected Outcome:\n",
    "By the end of this notebook, we will identify the best forecasting model for coffee prices and present actionable insights based on the findings.\n",
    "\n",
    "        Forecasting: generate forecasts for future I-CIP values using the best-performing model(s) and visualize the results to facilitate interpretation and decision-making.\n",
    "- 1 day\n",
    "- 5 days = 1 week\n",
    "- 21 days = 1 month\n",
    "\n",
    "\n",
    "(- 63 days = 3 months (1 quarter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b5168",
   "metadata": {},
   "source": [
    "### Importing relevant libraries for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c486ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "## cheking if keras/tensorflow are correclty installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd396c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd #dataframes \n",
    "import numpy as np #linear algebra\n",
    "import seaborn as sns #visualization\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "import scipy.stats as stats #statistical resources\n",
    "\n",
    "import matplotlib.pyplot as plt #visualisation \n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.model_selection import train_test_split # importing function to split the data training and test.\n",
    "from sklearn.preprocessing import MinMaxScaler # Import the MinMaxScaler module from sklearn.preprocessing library\n",
    "from sklearn.linear_model import LinearRegression # importing to performe linear regression. \n",
    "from sklearn.metrics import make_scorer, r2_score # Importing from Metrics module\n",
    "from sklearn.preprocessing import StandardScaler # standardize the data\n",
    "from sklearn import metrics # Metrics module from scikit-learn\n",
    "from sklearn.model_selection import GridSearchCV # importing for hyperparameter tunning\n",
    "from sklearn.metrics import mean_squared_error # importing mse\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential #last update in python causing dead kernel wehn importing keras functions?\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e8f09",
   "metadata": {},
   "source": [
    "# 1. Load data\n",
    "\n",
    "For the early stages of this experimentation present on the first notebook (Models2_copy), 1 year worth of data was available to collect, from 01Feb23 to 29Feb24.\n",
    "\n",
    "This section will review the original dataset compiled with data from feb 23 to feb 24.\n",
    "\n",
    "\n",
    "A few thingsobserved when importing the raw files from the data source: \n",
    "\n",
    "- Column mismatch: Assuming all files have the same column names and order. This could lead to errors when merging DataFrames with different structures. \n",
    "\n",
    "The data for each month is published separetely. Originally the 4 first months had different colum labels for the same data 'ICO Composite' , while the following months was simpler version as 'I-CIP'. For that diverson it was not possible to simply merge all dataframes into one. Dta cleaning/manipulation techniques of renamimbg and reorganising them chronologically were adopeted to reach the final dataset for the first year of data alocated in the 'icip_data' below. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331a3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the CSV file \n",
    "icip_data = pd.read_csv(\"icip_df.csv\")\n",
    "\n",
    "# View the first 5 rows\n",
    "icip_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41abde",
   "metadata": {},
   "source": [
    "Since then, ICO has released additional months that will be included in the main dataframe, considering the timeframe from march to september 2024 as a way to feed more data to the models with the expectation it could improve the results. These seven new files will be sorted by chronological order and have the same labels as the main one above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42756de2",
   "metadata": {},
   "source": [
    "### Importing  additional data from March/24 to September/24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# List all the files in the folder\n",
    "os.listdir(\"icip_24\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e7b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8486d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create for loop to import csv files from the folder with less comands.\n",
    "\n",
    "# create an empty list to store dfs\n",
    "dataframes = []\n",
    "\n",
    "# path to folder where csv files are (in this case same directory)\n",
    "folder_path = \"icip_24\"\n",
    "\n",
    "\n",
    "# to import CSV starting from the third row, skipping the first two\n",
    "def import_csv(filepath):\n",
    "    return pd.read_csv(filepath, skiprows=2)\n",
    "\n",
    "# Iterate through files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):  # Only consider CSV files\n",
    "        file_path = os.path.join(folder_path, file)  # Construct the full file path\n",
    "        dataframes.append(import_csv(file_path))  # Read CSV and append to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ef932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad605ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the lenght of the directory, how many files exist in the new folder\n",
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1135e0d1",
   "metadata": {},
   "source": [
    "Chcking the heading of the files to undertand how features are allocated in this first stage before combining the new 7 months to main dataframe\n",
    "\n",
    "The same issue appears with the heading names. So this time around it was decided to ignore the first 2 rows to avoid the unnamed labels and only import the actual data \n",
    "\n",
    ">Unnamed: 0\tUnnamed: 1\tColombian\tUnnamed: 3\tBrazilian\tUnnamed: 5\n",
    "\n",
    "\n",
    ">0\tNaN\tI-CIP\tNaN\tOther Milds\tNaN\tRobusta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if order of files correspond with the directory list, testing if loop is working\n",
    "dataframes[5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387462db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dataframes)\n",
    "#list of all dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8ce2b",
   "metadata": {},
   "source": [
    "To continue the project is necessary to make 2 adjustments in the second directory:\n",
    "- change the date format from \" 06-Jun\" to '%Y-%m-%d' format and apply this to all files in the \"Unnamed: 0\" collum which corresponds to date. This will enable a more smooth combination of the 2 dfs once all dates mantain the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: print the first DataFrame to check if the transformation worked\n",
    "print(dataframes[5].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac339ca",
   "metadata": {},
   "source": [
    "The code below will use the month mapping to rearranje data in the first colum from corresponding to dates.\n",
    "- first it will change the format from '**01-Jul**' to **''%Y-%m-%d''** through all months in the list of dataframes.\n",
    "- then will replace the dataframes with the correct format and set as 'datetime' type. \n",
    "- print the correct year for all dataframes and show the correct label for 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd76423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform the 'Unnamed: 0' date column for each DataFrame in the list and reorder columns\n",
    "def transform_date(dataframes, year):\n",
    "    month_mapping = {\n",
    "        'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "        'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "        'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "    }\n",
    "    \n",
    "    # Iterate over each DataFrame in the list\n",
    "    for i in range(len(dataframes)):\n",
    "        df = dataframes[i]\n",
    "        \n",
    "        # Print the columns to inspect if 'Unnamed: 0' exists or if the name is different\n",
    "        print(f\"Columns in DataFrame {i}: {df.columns}\")\n",
    "        \n",
    "        # Check if 'Unnamed: 0' exists, otherwise handle the column name differently\n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            # Apply the transformation to the 'Unnamed: 0' column to create full date strings\n",
    "            df['date'] = df['Unnamed: 0'].apply(\n",
    "                lambda x: '-'.join([str(year), month_mapping[x.split('-')[1]], x.split('-')[0]])\n",
    "            )\n",
    "            \n",
    "            # Convert the 'Date' column to datetime format\n",
    "            df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "            \n",
    "            # Drop the original 'Unnamed: 0' column\n",
    "            df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "            \n",
    "            # Reorder columns to place 'Date' first\n",
    "            columns = ['date'] + [col for col in df.columns if col != 'date']\n",
    "            dataframes[i] = df[columns]  # Replace the DataFrame with the reordered one\n",
    "        else:\n",
    "            print(f\"'Unnamed: 0' column not found in DataFrame {i}\")\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "# Apply the function to the list of DataFrames\n",
    "dataframes = transform_date(dataframes, 2024)\n",
    "\n",
    "# Test: print the first DataFrame to check if the column reordering worked\n",
    "print(dataframes[0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7fb40",
   "metadata": {},
   "source": [
    "## Splitting the date column to match main dataset\n",
    "\n",
    "This function is mainly to add the last 2 columns matching the original dataset which contains the year and m,onth of each date. This can also be used for some of exploratory plots in next sectiosn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add year and month columns to each DataFrame in the list\n",
    "def add_year_month_columns(dataframes):\n",
    "    for i in range(len(dataframes)):\n",
    "        df = dataframes[i]\n",
    "        \n",
    "        # Extract the year and month from the 'Date' column\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        \n",
    "        # Replace the DataFrame in the list with the new columns added\n",
    "        dataframes[i] = df\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "# Apply the function to the list of DataFrames\n",
    "dataframes = add_year_month_columns(dataframes)\n",
    "\n",
    "# checking if transformation worked in the dataframes list:\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2f829",
   "metadata": {},
   "source": [
    "### Define chronologic order for dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of DataFrames in the desired order\n",
    "dfs_in_order = [dataframes[3],dataframes[2],dataframes[4],dataframes[6],dataframes[5],dataframes[1],dataframes[0]]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "merged_df = pd.concat(dfs_in_order,ignore_index=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775441b6",
   "metadata": {},
   "source": [
    "from the info function displays the new data contains 152 observations across 8 columss from march 24 to september 24.\n",
    "the first colum shows dates in datetime format, followed by each category of coffee as well as the index values as floats. the added year and month number of each observation is in integer format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7df6bf",
   "metadata": {},
   "source": [
    "### Rename column names prior to merging both datasets\n",
    "\n",
    "This will enable to combine previous data from original dataset to have a bigger pool of observations to feed more data in the modeling part. Is expected the final dataset to combine data from feb/23 to sep/24\n",
    "\n",
    "- df1 = icip_data > contains the original dataset (Feb 2023 - Feb 2024)\n",
    "- df2 = merged_df > contains the new dataset (Mar 2024 - Sep 2024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = icip_data\n",
    "df2 = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the 'Date' column in both df1 and df2 is in datetime format\n",
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "df2['date'] = pd.to_datetime(df2['date'])\n",
    "\n",
    "# Rename the columns in df2 to match the structure of df1\n",
    "df2.columns = ['date', 'I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas', 'year', 'month']\n",
    "\n",
    "# Concatenate df1 and df2 into a single DataFrame\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Sort by the 'Date' column to ensure chronological order\n",
    "combined_df = combined_df.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a CSV file\n",
    "combined_df.to_csv('final_combined_data.csv', index=False)\n",
    "\n",
    "# Test: print the first few rows to verify the result\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e579854",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfee9b",
   "metadata": {},
   "source": [
    "The combined dataset on the correct stucture can help to make better explorations on the next sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52b5d1",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "\n",
    "This section aims to see how the data is presented, such as basic statistics and its distribution over time.  They can help to identify average levels, variability, and the range of values \n",
    "(large deviations might suggest volatility, which could be important for modeling in later stages of experimentation)\n",
    "\n",
    "\n",
    "### 2.1 Summary statistics and checking for missing values\n",
    "\n",
    "From the *info* fucntion we get 431 rows of numerical data, with the first colum showing values in datetime type followed by the ICO composite prices and each category of coffee used for the index. The last 2 columsn are the ones added earlier with the month and year of each data point. \n",
    "\n",
    "This analysis will help to interpret basic price dynamics before diving into more complex forecasting techniques, such as SaARIMA and LSTMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic info\n",
    "\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics for numeric columns \n",
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "print(combined_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfd2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "\n",
    "print(combined_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f491c5",
   "metadata": {},
   "source": [
    "## 2.2 Trends over time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7f577",
   "metadata": {},
   "source": [
    "### Plotting trends overtime to begin understanding how this new dataset is presented\n",
    "\n",
    "This also helps in highlighting any major shifts or trends which are quite pronounced in this dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f81958",
   "metadata": {},
   "source": [
    "### a. Comparing the different categories over time:\n",
    "\n",
    "Each category has a different weight to calculate the final composite. **(get data on this)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56743c02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot other categories of coffee over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(combined_df['date'], combined_df['I-CIP'], label='I-CIP')\n",
    "plt.plot(combined_df['date'], combined_df['colombian_milds'], label='Colombian Milds')\n",
    "plt.plot(combined_df['date'], combined_df['other_milds'], label='Other Milds')\n",
    "plt.plot(combined_df['date'], combined_df['brazilian_nat'], label='Brazilian Naturals')\n",
    "plt.plot(combined_df['date'], combined_df['robustas'], label='Robustas')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (US cents/lb)')\n",
    "plt.title('Coffee Types Trend Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44f9a0",
   "metadata": {},
   "source": [
    "\n",
    "changing labels for date axis for easier visualisation (ie from 2023-03 to MAR 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Define only the columns you want to plot (excluding the last two columns)\n",
    "columns_to_plot = ['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate over the selected columns and plot each one\n",
    "for column in columns_to_plot:\n",
    "    plt.plot(combined_df['date'], combined_df[column], label=column)\n",
    "\n",
    "# Customize x-axis to show months (use date format for better readability)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Price (US cents/lb)')\n",
    "plt.title('Price Fluctuations of ICO Composite Indicator and Coffee Groups Over Time')\n",
    "plt.legend()\n",
    "\n",
    "# Format the x-axis labels to show the month name with better spacing\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # Shows every 3rd month\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "\n",
    "plt.xticks(rotation=45)  # Rotate for better readability\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d7822",
   "metadata": {},
   "source": [
    "###  Setting the date column as index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new variable for merged_df and reseting date as the index for building time series in later stages\n",
    "# Set 'date' column as index\n",
    "combined_df.set_index('date', inplace=True)\n",
    "\n",
    "#check output\n",
    "icip_df = combined_df\n",
    "icip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01de80d",
   "metadata": {},
   "source": [
    "###  Checking the range of dataset: \n",
    "\n",
    "\n",
    "With the dates as indext we can check the range of the dataset: \n",
    "\n",
    "- 607 days however the data collected is at a frequency of BUSINESS DAYS, excluding weekends and holidays, which would account for the difference between the total days (607) and the number of observations (431)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking how many days are present in the dataset\n",
    "\n",
    "print(f'Dataframe contains prices between {icip_df.index.min()} {icip_df.index.max()}')\n",
    "print(f'Total Days = {icip_df.index.max() - icip_df.index.min()} days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a7b8c",
   "metadata": {},
   "source": [
    "### Once the date is set as index, is possible to measure the range an frequency of data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure the index is set at datetime \n",
    "icip_df.index = pd.to_datetime(icip_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the range, confirm the frequency of the index\n",
    "print(icip_df.index.freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5b3a2",
   "metadata": {},
   "source": [
    "A freq marked as 'None' makes python treat the date as irregular. Manually setting the frquency as Business days since the frequancy is not really defined.\n",
    "This can have a series of benefits:\n",
    "- Align  data with time-based operations.\n",
    "- Perform accurate rolling calculations and time series decomposition.\n",
    "- Handle missing data systematically.\n",
    "- Use advanced time series models and resampling.\n",
    "\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/version/0.16/timeseries.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75768777",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df = icip_df.asfreq('B')  # B stands for Business Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1958f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the range, confirm the frequency of the index\n",
    "print(icip_df.index.freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e335a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fec84a",
   "metadata": {},
   "source": [
    "After setting the freqeuncy as busines days, the index automatically added 3 more empty rows to the dataset.\n",
    "index shows **434** entries while non null count is **431**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99eac29",
   "metadata": {},
   "source": [
    "### Value distribution across categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b26ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define colors from the Set2 palette\n",
    "colors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854']\n",
    "\n",
    "# Specify the columns you want to include in the boxplot\n",
    "selected_columns = ['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']  \n",
    "\n",
    "# Create boxplot traces\n",
    "box_traces = []\n",
    "for i, column in enumerate(selected_columns):\n",
    "    color = colors[i % len(colors)] \n",
    "    box_trace = go.Box(y=icip_df[column], name=column, marker=dict(color=color))\n",
    "    box_traces.append(box_trace)\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(title='Boxplot by Column', yaxis=dict(title='Value'), xaxis=dict(title='Variable'))\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure(data=box_traces, layout=layout)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619911d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3eddf46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bar plot of mean values for each column\n",
    "plt.figure(figsize=(10, 6))\n",
    "icip_df[selected_columns].mean().plot(kind='bar', color='skyblue')\n",
    "plt.title('Mean Values of Each Variable')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Mean')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix for Coffee Prices\n",
    "correlation_matrix = icip_df[['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.matshow(correlation_matrix, cmap='coolwarm', fignum=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation Matrix of Coffee Prices\")\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47eeda7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  b. ICO Composite Indicator Price (I-CIP) \n",
    "\n",
    "I-CIP is the main feature to be used for this analysis. \n",
    " \n",
    " \n",
    "Thereâ€™s a clear upward trend in the I-CIP index from early 2023 to late 2024. This suggests that coffee prices, as represented by I-CIP, have generally increased over this period.\n",
    "\n",
    "Initial Fluctuations (Early to Mid-2023): In the first part of the timeline (early to mid-2023), the I-CIP prices exhibit noticeable fluctuations, with a few peaks and troughs. This suggests some instability in the market, with prices rising and falling frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4779103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot I-CIP over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(icip_df['I-CIP'], label='I-CIP')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('I-CIP')\n",
    "plt.title('I-CIP Trend Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c7c57",
   "metadata": {},
   "source": [
    "looking closer at the gap in thi line plot:\n",
    "between dec 23 and jan 24 there seems to have a missing value there\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88962c4a",
   "metadata": {},
   "source": [
    "### 2.2.1\n",
    "\n",
    "### a. Checking monthly seasonality\n",
    "\n",
    "This plot shows the seasonalityvariation in prices over the years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month from the index\n",
    "\n",
    "## plot only for 2023 and plot a separate for 2024\n",
    "#are variations in price the same in both year????\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "icip_df['year'] = icip_df.index.year\n",
    "icip_df['month'] = icip_df.index.month_name().str[:3]  # This will give  the three-letter month abbreviation.\n",
    "\n",
    "# Draw Plot\n",
    "plt.figure(figsize=(12, 7), dpi=80)\n",
    "sns.boxplot(x='month', y='I-CIP', data=icip_df)\n",
    "\n",
    "# Set Title\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices of 2023/2024\\n(The Seasonality)', fontsize=18)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e35252",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f562f4",
   "metadata": {},
   "source": [
    "##  b. Separating values montly per each year\n",
    "\n",
    "Re-arranging the order for yearly plots since the data is not complete, since we have for 2023: Feb to Dec and 2024: Jan to Sep, the labels are done manually below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by year\n",
    "df_2023 = icip_df[icip_df['year'] == 2023]\n",
    "df_2024 = icip_df[icip_df['year'] == 2024]\n",
    "\n",
    "# Plot for 2023\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='month', y='I-CIP', data=df_2023, order=['Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices in 2023\\n(The Seasonality)', fontsize=18)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('I-CIP Price (In US cents/lb)')\n",
    "plt.show()\n",
    "\n",
    "# Plot for 2024 (up to September)\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='month', y='I-CIP', data=df_2024, order=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep'])\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices in 2024\\n(The Seasonality)', fontsize=18)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('I-CIP Price (In US cents/lb)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e946343a",
   "metadata": {},
   "source": [
    "### c. Year-over-Year Monthly Comparison:\n",
    "\n",
    "This plot aims to compare the avarage of icip prices in 2023 and 2024, to highlight differences in each year.\n",
    "\n",
    "Overall, the year of 24 is represented by higher average, confirming the upward trend seen in line plots in item c.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae978a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the month order to ensure chronological sorting\n",
    "month_order = list(calendar.month_abbr[1:])  # ['Jan', 'Feb', ..., 'Dec']\n",
    "icip_df['month'] = pd.Categorical(icip_df['month'], categories=month_order, ordered=True)\n",
    "\n",
    "# Monthly average comparison\n",
    "monthly_avg = icip_df.groupby(['year', 'month'])['I-CIP'].mean().unstack(level=0)\n",
    "monthly_avg.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Average Monthly I-CIP Prices in 2023 vs 2024')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average I-CIP Price (in US cents/lb)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3962408",
   "metadata": {},
   "source": [
    "months appear in alphabetical order instead of chronological? >>> **adjust\\!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly Average Plot\n",
    "monthly_avg = icip_df['I-CIP'].resample('M').mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(monthly_avg, label=\"Monthly Average I-CIP\")\n",
    "plt.title(\"Monthly Average I-CIP Prices\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"I-CIP Price (In US cents/lb)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e6f92",
   "metadata": {},
   "source": [
    "### d.  Monthly Average I-CIP Prices with Regional Harvest Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127e560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the month order to ensure chronological sorting\n",
    "#month_order = list(calendar.month_abbr[1:])  # ['Jan', 'Feb', ..., 'Dec']\n",
    "#icip_df['month'] = pd.Categorical(icip_df['month'], categories=month_order, ordered=True)\n",
    "\n",
    "# Group by both year and month to keep month names in chronological order\n",
    "monthly_avg_df = icip_df.groupby([icip_df.index.year, 'month'])['I-CIP'].mean().unstack(level=0)\n",
    "\n",
    "# Create the plot with annotations of harvest \n",
    "plt.figure(figsize=(14, 7))\n",
    "monthly_avg_df.plot(kind='bar', color=['skyblue', 'salmon'], ax=plt.gca())\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average I-CIP Price (in US cents/lb)')\n",
    "plt.title('Monthly Average I-CIP Prices by Year with Harvest Annotations')\n",
    "\n",
    "# Annotate months with regional harvests (approximately)\n",
    "harvest_annotations = {\n",
    "    'Jan': 'South America, Africa',\n",
    "    'Feb': 'South America, Africa',\n",
    "    'Mar': 'South America',\n",
    "    'Apr': 'Central America, South America',\n",
    "    'May': 'Asia',\n",
    "    'Jun': 'South America, Africa, Asia',\n",
    "    'Jul': 'Asia, Africa',\n",
    "    'Aug': 'Asia, Africa',\n",
    "    'Sep': 'Asia',\n",
    "    'Oct': 'South America, Africa, Asia',\n",
    "    'Nov': 'Central America, Africa',\n",
    "    'Dec': 'South America, Africa'\n",
    "}\n",
    "\n",
    "# Add annotations at 45-degree angle for readability\n",
    "for month_idx, (month, regions) in enumerate(harvest_annotations.items()):\n",
    "    plt.text(month_idx - 0.15, monthly_avg_df.loc[month].max() + 5, \n",
    "             regions, ha='center', rotation=45, color='black', fontsize=8)\n",
    "\n",
    "# Adjust x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Year\", loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# harverst dates extracted from \n",
    "#Source: https://coffeehunter.com/coffee-seasonality/ accessed on 30/10\n",
    "# https://www.fairmountaincoffee.com/category-s/102.htm accessedon 30/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc98a72",
   "metadata": {},
   "source": [
    "#### Heatmap of Monthly Price Averages \n",
    "\n",
    "The heatmat below aims to identify months where prices tend to dip or spike, then cross-reference with known harvest periods. The color intensity provides a quick overview of price levels each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741dfecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for monthly averages\n",
    "monthly_avg_df = icip_df.groupby([icip_df.index.year, icip_df.index.month])['I-CIP'].mean().unstack()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(monthly_avg_df, annot=True, cmap=\"YlGnBu\", fmt=\".1f\", linewidths=0.5)\n",
    "plt.title('Monthly I-CIP Price Averages (in US cents/lb)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c243f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a307c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281a40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123d4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43f79099",
   "metadata": {},
   "source": [
    "##  Value distribution across categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0449bbe",
   "metadata": {},
   "source": [
    "###### plot monthy only by category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of the original DataFrame without 'year' and 'month' columns\n",
    "copy = icip_df.drop(columns=['year', 'month']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208fbd3",
   "metadata": {},
   "source": [
    "the last 2 columns were only added to facilite some of the montkly plots, ill copy the main data as a separate dataframe for more statistical measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90cf07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7177048",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c84f05",
   "metadata": {},
   "source": [
    "## 2.3 Checking correlation across categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = copy.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213d414",
   "metadata": {},
   "source": [
    ">>The matrix highlights that colombian_milds and other_milds are closely related and likely to be primary influencers on the I-CIP index. In contrast, robustas exhibits more independence from other types. These relationships provide a foundation for selecting variables in a forecasting model, where incorporating highly correlated categories might improve predictive accuracy.<<"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03abd7ff",
   "metadata": {},
   "source": [
    "## 2.4 Distribuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915014cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing some of the aesthetics... adding in a kernel desnity estimate:\n",
    "ax = sns.histplot(copy['I-CIP'], kde=True, color ='green')\n",
    "ax.set(xlabel='i-cip',\n",
    "       ylabel='Frequency',\n",
    "       title =\"ICO Composite Prices  Histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee14ed4",
   "metadata": {},
   "source": [
    "From the histogram above, we can see the data doensnt show a normal distribution of values, the graph below a kde is added to the plot to make it even clearer. The Bimodal distribution indicates the presence of two distinct groups within the dataset, each with its own central tendency. (two populations in the sample)\n",
    "\n",
    "\n",
    "This often suggests that this dataset may be influenced by two different underlying processes or conditions. In terms of comodity markets, for example, a distribution like this one might reflect different market dynamics or consumption patterns across two separate seasons or market segments. (**find ref**) \n",
    "\n",
    "The next step is to consider whether these modes represent meaningful segments that require additional analysis or modeling strategies. By fitting a Gaussian Mixture Model or exploring the data's skewness and kurtosis, we can dive deeper into understanding the characteristics of these subgroups, which can give more insight son how to proceed to tailoring the forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6d7ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create histogram\n",
    "fig = px.histogram(copy['I-CIP'], x='I-CIP', nbins=30) #changing the number of bins makes the curve clearer, 10, 20 were also used\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='I-CIP Histogram',\n",
    "    xaxis_title='Price',\n",
    "    yaxis_title='Frequency'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1659737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with two subplots to check for distribution\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot histogram of icip prices in the first subplot\n",
    "axs[0].hist(copy['I-CIP'], bins=10)\n",
    "axs[0].set_title('Histogram of I-CIP values')\n",
    "axs[0].set_xlabel('Value')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot probability plot of prices in the second subplot\n",
    "stats.probplot(copy['I-CIP'], dist='norm', plot=axs[1])\n",
    "axs[1].set_title('Probability Plot of Values')\n",
    "\n",
    "# Adjust the layout to avoid overlapping titles and labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65fb719",
   "metadata": {},
   "source": [
    "### checking skewness of data\n",
    "\n",
    "from the plot below we see the data is presenting a bimodal distribuition, which indicates the non normality of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb219d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Visual Inspection\n",
    "plt.hist(copy['I-CIP'], bins=30, alpha=0.5, color='blue', density=True)\n",
    "copy['I-CIP'].plot(kind='density', color='green')\n",
    "plt.title('Histogram and Density Plot')\n",
    "plt.show()\n",
    "\n",
    "# Check skewness and kurtosis\n",
    "data_skewness = skew(copy['I-CIP'])\n",
    "data_kurtosis = kurtosis(copy['I-CIP'])\n",
    "print(f\"Skewness: {data_skewness}\")\n",
    "print(f\"Kurtosis: {data_kurtosis}\")\n",
    "\n",
    "# Fitting a Gaussian Mixture Model >>> cant fit this model when there are Nan values in the datatset\n",
    "#gmm = GaussianMixture(n_components=2, random_state=0).fit(np.expand_dims(copy['I-CIP'], 1))\n",
    "#gmm_scores = gmm.score_samples(np.expand_dims(copy['I-CIP'], 1))\n",
    "\n",
    "#print(f\"GMM Converged: {gmm.converged_}\") # Check if the algorithm has converged\n",
    "#print(f\"GMM Weights: {gmm.weights_}\") # Gives the weights of the two components\n",
    "\n",
    "# Plotting the GMM results\n",
    "#plt.hist(copy['I-CIP'], bins=30, alpha=0.5, color='blue', density=True)\n",
    "#plt.plot(np.linspace(min(copy['I-CIP']), max(copy['I-CIP']), len(gmm_scores)), np.exp(gmm_scores), color='red')\n",
    "#plt.title('Histogram with GMM-estimated Density')\n",
    "#plt.show()\n",
    "\n",
    "#Source: https://builtin.com/articles/gaussian-mixture-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a8ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "784fa41c",
   "metadata": {},
   "source": [
    "\n",
    "**ValueError: Input X contains NaN.** \n",
    "GaussianMixture does not accept missing values encoded as NaN natively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118a95a",
   "metadata": {},
   "source": [
    "### checking for missing dates for determine the right frequency\n",
    "\n",
    "\n",
    " This code below extracs the exact dates missing from the entire range for business days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c51b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a date range for 366 days from the start of your data\n",
    "# Adjust the period accordingly if you have data spanning multiple years or a different time frame\n",
    "start_date = copy.index.min()\n",
    "end_date = copy.index.max()  \n",
    "\n",
    "\n",
    "# Generate a range of business days within this period\n",
    "business_days = pd.bdate_range(start=start_date, end=end_date)\n",
    "\n",
    "# Now compare the business_days with your DataFrame's index to find out missing dates\n",
    "missing_dates = business_days.difference(copy.index)\n",
    "\n",
    "print(f\"Total number of expected business days: {len(business_days)}\")\n",
    "print(f\"Total number of actual days in data: {copy.shape[0]}\")\n",
    "print(f\"Total number of missing dates: {len(missing_dates)}\")\n",
    "print(\"Missing dates are:\")\n",
    "print(missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bae749",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3abb5b9",
   "metadata": {},
   "source": [
    "The code above says there are no missing dates in the dataframe, however, it still shows there are a few NaN values. This is confirmed below with the *is null* code that shows a total of 3 missing values across all columns:  out of the 434 entries computed by the datetime index, ranging from 2023-02-01 to 2024-09-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41092b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "copy.info()\n",
    "print(copy.shape)\n",
    "print(copy.isnull().sum())\n",
    "copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695439a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df = copy.isna()\n",
    "print(nan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows = copy.isna().any(axis=1)\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c92348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter rows with nan values\n",
    "nan_rows = copy[copy.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c77976",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(copy.isna(), cbar=False, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dates with missing data\n",
    "nan_rows.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ba323",
   "metadata": {},
   "source": [
    "## IDENTIFIED MISSING DATES \n",
    "## INTERPOLATE FOR THE TIMESERIES\n",
    "\n",
    "MODELS CANT HANDLE MISSING DATA \n",
    "\n",
    "**Missing dates**  \n",
    "- **'2023-12-25'**: December 25th: Christmas Day\n",
    "- **'2023-12-26'**: December 26th: Often observed as Boxing Day or a Christmas holiday extension\n",
    "- **'2024-01-01'**: New Year's Day\n",
    "\n",
    "From the early eda, it was observed no missing values per se, however, for a timeseries analysis, the date in the index must be following the correct sequence (\"order?\"). it's the main characteristic of modeling this type of data. So eventhough there were no missing values (all dates had values) the date range was incomplete. \n",
    "\n",
    "**\n",
    "Another insight from this is that the composite price is published by the ICO even in regular holidays? \n",
    "The price composite is agregated values from US and EU (germany+france), this could be an indicative of why there are only 3 dates not in their database. \n",
    "\n",
    "For this project, lets add these three dates by chacking the moving average and the foward fill methods.\n",
    "There is also an alternatite to model the holiday effect on trend/seasonality of the data by including special events or dummy variables (but for the purpose of this experimentation, it wont be necessary because only 3 dates are a relatively small number of holidays to be considered in modeling. \n",
    "\n",
    "\n",
    "This section will compare 3 different methods for interpolation of missing values in the ICIP dataset\n",
    "\n",
    "- foward fill\n",
    "- backward fill \n",
    "- linear interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86720b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Fill\n",
    "df_ffill = copy.ffill()\n",
    "\n",
    "# Backward Fill\n",
    "df_bfill = copy.bfill()\n",
    "\n",
    "# Linear Interpolation \n",
    "df_interpolate = copy.interpolate()\n",
    "\n",
    "# Get the indices of rows with NaN values in the original DataFrame\n",
    "nan_indices = copy[copy.isna().any(axis=1)].index\n",
    "\n",
    "# compare the results for the `I-CIP` column at the NaN rows\n",
    "print(\"Forward Fill:\")\n",
    "print(df_ffill['I-CIP'].loc[nan_indices])\n",
    "\n",
    "print(\"\\nBackward Fill:\")\n",
    "print(df_bfill['I-CIP'].loc[nan_indices])\n",
    "\n",
    "print(\"\\nInterpolation:\")\n",
    "print(df_interpolate['I-CIP'].loc[nan_indices])\n",
    "\n",
    "## https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b80a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d018932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d40af2b9",
   "metadata": {},
   "source": [
    "Interpolation (linear) is typically the most appropriate for time series data because it provides a gradual transition between values, which better mimics natural fluctuations in price indices.\n",
    "Forward and Backward Fill are simpler methods and may be suitable if you believe that missing values should hold steady from a previous or upcoming value, but they can introduce unrealistic flat segments.\n",
    "\n",
    "\n",
    "https://medium.com/@aaabulkhair/data-imputation-demystified-time-series-data-69bc9c798cb7\n",
    "\n",
    "https://365datascience.com/tutorials/time-series-analysis-tutorials/pre-process-time-series-data/\n",
    "\n",
    "### Adding linear interpolation to main dataset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48163393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply linear interpolation to fill missing values in the DataFrame\n",
    "copy_interpolated = copy.interpolate(method='linear')\n",
    "\n",
    "# Verify that there are no remaining NaN values\n",
    "print(copy_interpolated.isna().sum())\n",
    "\n",
    "# Display a sample of the DataFrame to check the interpolation results\n",
    "copy_interpolated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply imputation methods\n",
    "df1['price_forward_fill'] = df1['I-CIP'].ffill()\n",
    "df1['price_backward_fill'] = df1['I-CIP'].bfill()\n",
    "df1['price_interpolate'] = df1['I-CIP'].interpolate()\n",
    "\n",
    "# Visual comparison of imputation methods\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, sharex=True, figsize=(10, 12))\n",
    "plt.rcParams.update({'xtick.bottom' : False})\n",
    "\n",
    "\n",
    "# Original data with missing points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "axes[0].plot(df.index, df['I-CIP'], label='Original Data', color='blue', linestyle='-', marker='o')\n",
    "#axes[0].plot(df1.index, df1['I-CIP'], label='Actual', color='red', style=\".-\")\n",
    "axes[0].set_title('Original Data with Missing Points')\n",
    "axes[0].legend([\"Missing Data\", \"Available Data\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Forward Fill\n",
    "axes[1].plot(df1.index, df1['price_forward_fill'], label='Forward Fill', color='orange', linestyle='--', marker='x')\n",
    "axes[1].set_title('Forward Fill')\n",
    "axes[1].legend()\n",
    "\n",
    "#\n",
    "# Backward Fill\n",
    "axes[2].plot(df1.index, df1['price_backward_fill'], label='Backward Fill', color='red', linestyle='--', marker='+')\n",
    "axes[2].set_title('Backward Fill')\n",
    "axes[2].legend()\n",
    "\n",
    "\n",
    "# Interpolation\n",
    "axes[3].plot(df1.index, df1['price_interpolate'], label='Interpolation', color='green', linestyle='--', marker='s')\n",
    "axes[3].set_title('Interpolation')\n",
    "axes[3].legend()\n",
    "\n",
    "\n",
    "\n",
    "#plot\n",
    "plt.title('Comparison of Imputation Methods')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8222a5",
   "metadata": {},
   "source": [
    "### Weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e807ac2",
   "metadata": {},
   "source": [
    "\n",
    "ICO Composite Indicator Price (I-CIP) is calculated using a weighted average of four coffee groups: Colombian Milds, Other Milds, Brazilian Naturals, and Robustas, with each group contributing a specific weight to the calculation:\n",
    "\n",
    "- Colombian Milds: 12%\n",
    "- Other Milds: 21%\n",
    "- Brazilian Naturals: 30%\n",
    "- Robustas: 37%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea66f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(copy_interpolated.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c73be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20461938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weighted I-CIP\n",
    "copy_interpolated['weighted_I-CIP'] = (0.12 * copy_interpolated['colombian_milds'] +\n",
    "                                 0.21 * copy_interpolated['other_milds'] +\n",
    "                                 0.30 * copy_interpolated['brazilian_nat'] +\n",
    "                                 0.37 * copy_interpolated['robustas'])\n",
    "\n",
    "# Compare weighted I-CIP with actual I-CIP\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(copy_interpolated.index, copy_interpolated['I-CIP'], label='Actual I-CIP')\n",
    "plt.plot(copy_interpolated.index, copy_interpolated['weighted_I-CIP'], label='Weighted I-CIP', linestyle='--')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('I-CIP')\n",
    "plt.title('Actual I-CIP vs Weighted I-CIP')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190816c0",
   "metadata": {},
   "source": [
    "#### Lag plots\n",
    "understanding the entropy of icip prices and how correlated they are\n",
    "\n",
    "the scatter plot bellow shows the relationship between observations and their lags.\n",
    "\"as the lag increases, the correlation between the time series and its lags generally decreases.\"\n",
    "\n",
    "Some sort of autocorrelation in the data is visible in lag 1, (t+1). A strong linear relationship indicates a high correlation between an observation and its immediate predecessor. a similar pattern is observed in lag2, with a few datapoints begining to get apart. Lags 3 and 4 are already more spreaded, meaning the correlation between values is also decreasing as the interval between lags grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f082ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "plt.rcParams.update({'ytick.left' : False, 'axes.titlepad':10})\n",
    "\n",
    "lp = icip_df['I-CIP']\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)\n",
    "for i, ax in enumerate(axes.flatten()[:4]):\n",
    "    lag_plot(lp, lag=i+1, ax=ax, c='firebrick')\n",
    "    ax.set_title('Lag ' + str(i+1))\n",
    "\n",
    "    \n",
    "fig.suptitle('Lag Plots of I-CIP prices \\n(Points get wide and scattered with increasing lag -> lesser correlation)\\n', y=1.15)    \n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of lags for 1 month\n",
    "number_of_lags = 21\n",
    "\n",
    "# Create subplots with 3 columns\n",
    "fig, axes = plt.subplots(nrows=7, ncols=3, figsize=(15, 20))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Generate a lag plot for each lag\n",
    "for i in range(1, number_of_lags + 1):\n",
    "    lag_plot(icip_df['I-CIP'], lag=i, ax=axes[i-1])\n",
    "    axes[i-1].set_title(f'Lag {i}')\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfc70e",
   "metadata": {},
   "source": [
    "### Rolling average / Rolling standard deviation\n",
    "\n",
    "\n",
    "Rolling Mean: The rolling mean is the average of the previous observation window, where the window consists of a series of values from the time series data. Computing the mean for each ordered window. This can significantly help minimize noise in time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc708917",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rolling Statistics at different periods\n",
    "window_sizes = [5, 21, 63]  # A week, a month, a quarter,  (approximately)\n",
    "data_rolling = copy_interpolated['I-CIP']  \n",
    "\n",
    "for window in window_sizes:\n",
    "    rolling_mean = copy_interpolated['I-CIP'].rolling(window=window).mean()\n",
    "    rolling_std = copy_interpolated['I-CIP'].rolling(window=window).std()\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(copy_interpolated['I-CIP'].index, copy['I-CIP'], label='Original')\n",
    "    plt.plot(rolling_mean.index, rolling_mean, label=f'Rolling Mean (window={window})')\n",
    "    plt.plot(rolling_std.index, rolling_std, label=f'Rolling Std Dev (window={window})')\n",
    "    plt.title(f'Rolling Mean and Standard Deviation (window size = {window})')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e459c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_interpolated.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d15706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303050b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e159ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b8619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd75f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997503bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064769ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print out results in customised manner\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "def kpss_test(timeseries):\n",
    "    print ('Results of KPSS Test:')\n",
    "    kpsstest = kpss(timeseries, regression='c', nlags=\"auto\")\n",
    "    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','#Lags Used'])\n",
    "    for key,value in kpsstest[3].items():\n",
    "        kpss_output['Critical Value (%s)'%key] = value\n",
    "    print (kpss_output)\n",
    "\n",
    "# Call the function and run the test\n",
    "\n",
    "kpss_test(copy_interpolated['I-CIP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70ad47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9a56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2267ccc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Seasonal decompositions with different periods.\n",
    "\n",
    "periods = [63, 21, 5]  # Quartely, Monthly and Weekly (considering business days)\n",
    "# Function to generate the plots for all periods.\n",
    "for period in periods:\n",
    "    decompositions = seasonal_decompose(copy_interpolated['I-CIP'], model='additive', period=period)\n",
    "\n",
    "    # Plotting the components of the decomposition\n",
    "    plt.rcParams.update({'figure.figsize': (8,8)})\n",
    "    print(f\"Seasonal Decomposition with Period = {period}\")\n",
    "    decompositions.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2cbee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79f07ea",
   "metadata": {},
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f7812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
