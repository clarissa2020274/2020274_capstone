{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f1b685",
   "metadata": {},
   "source": [
    "## MSc Data Analytics - Capstone Project\n",
    "\n",
    "#### Predictive Analysis in the Coffee Market: Using Deep Learning to predict coffee prices.\n",
    "Student id: 2020274 Clarissa Cardoso\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This notebook aims to analyse historical coffee price indices and develop a predictive model for future price trends. The focus is on using data from the ICO (International Coffee Organization), particularly indices like I-CIP, that combines prices of Colombian Milds, Other Milds, Brazilian Naturals, and Robustas.\n",
    "\n",
    "### Dataset:\n",
    "The dataset used in this analysis consists of historical coffee price data, with daily observations for business days. Prices are expressed in cents of USD per lb. \n",
    "\n",
    "\n",
    "The data utilized in this project is sourced from the International Coffee Organization's (ICO) Public Market Information, which provides the I-CIP values free of charge.\n",
    "\n",
    "For the early stages of the experimentation, 1 year worth of data was available to collect, from 01Feb23 to 29Feb24, which is present on a separate notebook (2020274_capstone_EDA_Models 2.ipynb). In this notebook, recent data from March to September 2024 were added to expand insights and feed more datapoints to modelling stage. \n",
    "\n",
    "\n",
    "### Objectives:\n",
    "1. Clean and preprocess the dataset for missing values and inconsistencies.\n",
    "2. Explore the time-series behavior of coffee prices through visualizations.\n",
    "3. Implement various forecasting models to predict future price trends, including traditional statistical models (e.g., ARIMA/Sarima) and deep learning algorithms (e.g., LSTM neural networks).\n",
    "4. Compare model performance using key metrics (e.g., RMSE, MAE).\n",
    "\n",
    "\n",
    "### Expected Outcome:\n",
    "By the end of this notebook, we will identify the best forecasting model for coffee prices and present actionable insights based on the findings.\n",
    "\n",
    "        Forecasting: generate forecasts for future I-CIP values using the best-performing model(s) and visualize the results to facilitate interpretation and decision-making.\n",
    "- 1 day\n",
    "- 5 days = 1 week\n",
    "- 21 days = 1 month\n",
    "\n",
    "\n",
    "(- 63 days = 3 months (1 quarter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b5168",
   "metadata": {},
   "source": [
    "### Importing relevant libraries for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c486ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "## cheking if keras/tensorflow are correclty installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd396c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd #dataframes \n",
    "import numpy as np #linear algebra\n",
    "import seaborn as sns #visualization\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "import scipy.stats as stats #statistical resources\n",
    "\n",
    "import matplotlib.pyplot as plt #visualisation \n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.model_selection import train_test_split # importing function to split the data training and test.\n",
    "from sklearn.preprocessing import MinMaxScaler # Import the MinMaxScaler module from sklearn.preprocessing library\n",
    "from sklearn.linear_model import LinearRegression # importing to performe linear regression. \n",
    "from sklearn.metrics import make_scorer, r2_score # Importing from Metrics module\n",
    "from sklearn.preprocessing import StandardScaler # standardize the data\n",
    "from sklearn import metrics # Metrics module from scikit-learn\n",
    "from sklearn.model_selection import GridSearchCV # importing for hyperparameter tunning\n",
    "from sklearn.metrics import mean_squared_error # importing mse\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential #last update in python causing dead kernel wehn importing keras functions?\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e8f09",
   "metadata": {},
   "source": [
    "# 1. Load data\n",
    "\n",
    "For the early stages of this experimentation present on the first notebook (Models2_copy), 1 year worth of data was available to collect, from 01Feb23 to 29Feb24.\n",
    "\n",
    "This section will review the original dataset compiled with data from feb 23 to feb 24.\n",
    "\n",
    "\n",
    "A few thingsobserved when importing the raw files from the data source: \n",
    "\n",
    "- Column mismatch: Assuming all files have the same column names and order. This could lead to errors when merging DataFrames with different structures. \n",
    "\n",
    "The data for each month is published separetely. Originally the 4 first months had different colum labels for the same data 'ICO Composite' , while the following months was simpler version as 'I-CIP'. For that diverson it was not possible to simply merge all dataframes into one. Dta cleaning/manipulation techniques of renamimbg and reorganising them chronologically were adopeted to reach the final dataset for the first year of data alocated in the 'icip_data' below. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331a3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the CSV file \n",
    "icip_data = pd.read_csv(\"icip_df.csv\")\n",
    "\n",
    "# View the first 5 rows\n",
    "icip_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca82f8",
   "metadata": {},
   "source": [
    "Since then, ICO has released additional months that will be included in the main dataframe, considering the timeframe from march to september 2024 as a way to feed more data to the models with the expectation it could improve the results. These seven new files will be sorted by chronological order and have the same labels as the main one above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42756de2",
   "metadata": {},
   "source": [
    "### Importing  additional data from March/24 to September/24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# List all the files in the folder\n",
    "os.listdir(\"icip_24\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e7b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8486d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create for loop to import csv files from the folder with less comands.\n",
    "\n",
    "# create an empty list to store dfs\n",
    "dataframes = []\n",
    "\n",
    "# path to folder where csv files are (in this case same directory)\n",
    "folder_path = \"icip_24\"\n",
    "\n",
    "\n",
    "# to import CSV starting from the third row, skipping the first two\n",
    "def import_csv(filepath):\n",
    "    return pd.read_csv(filepath, skiprows=2)\n",
    "\n",
    "# Iterate through files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):  # Only consider CSV files\n",
    "        file_path = os.path.join(folder_path, file)  # Construct the full file path\n",
    "        dataframes.append(import_csv(file_path))  # Read CSV and append to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ef932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad605ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the lenght of the directory, how many files exist in the new folder\n",
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1135e0d1",
   "metadata": {},
   "source": [
    "Chcking the heading of the files to undertand how features are allocated in this first stage before combining the new 7 months to main dataframe\n",
    "\n",
    "The same issue appears with the heading names. So this time around it was decided to ignore the first 2 rows to avoid the unnamed header and only collect the data \n",
    "\n",
    "Unnamed: 0\tUnnamed: 1\tColombian\tUnnamed: 3\tBrazilian\tUnnamed: 5\n",
    "0\tNaN\tI-CIP\tNaN\tOther Milds\tNaN\tRobusta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if order of files correspond with the directory list, testing if loop is working\n",
    "dataframes[5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387462db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dataframes)\n",
    "#list of all dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8ce2b",
   "metadata": {},
   "source": [
    "To continue the project is necessary to make 2 adjustments in the second directory:\n",
    "- change the date format from \" 06-Jun\" to '%Y-%m-%d' format and apply this to all files in the \"Unnamed: 0\" collum which corresponds to date. This will enable a more smooth combination of the 2 dfs once all dates mantain the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: print the first DataFrame to check if the transformation worked\n",
    "print(dataframes[5].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd76423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform the 'Unnamed: 0' date column for each DataFrame in the list and reorder columns\n",
    "def transform_date(dataframes, year):\n",
    "    month_mapping = {\n",
    "        'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "        'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "        'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "    }\n",
    "    \n",
    "    # Iterate over each DataFrame in the list\n",
    "    for i in range(len(dataframes)):\n",
    "        df = dataframes[i]\n",
    "        \n",
    "        # Print the columns to inspect if 'Unnamed: 0' exists or if the name is different\n",
    "        print(f\"Columns in DataFrame {i}: {df.columns}\")\n",
    "        \n",
    "        # Check if 'Unnamed: 0' exists, otherwise handle the column name differently\n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            # Apply the transformation to the 'Unnamed: 0' column to create full date strings\n",
    "            df['date'] = df['Unnamed: 0'].apply(\n",
    "                lambda x: '-'.join([str(year), month_mapping[x.split('-')[1]], x.split('-')[0]])\n",
    "            )\n",
    "            \n",
    "            # Convert the 'Date' column to datetime format\n",
    "            df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "            \n",
    "            # Drop the original 'Unnamed: 0' column\n",
    "            df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "            \n",
    "            # Reorder columns to place 'Date' first\n",
    "            columns = ['date'] + [col for col in df.columns if col != 'date']\n",
    "            dataframes[i] = df[columns]  # Replace the DataFrame with the reordered one\n",
    "        else:\n",
    "            print(f\"'Unnamed: 0' column not found in DataFrame {i}\")\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "# Apply the function to the list of DataFrames\n",
    "dataframes = transform_date(dataframes, 2024)\n",
    "\n",
    "# Test: print the first DataFrame to check if the column reordering worked\n",
    "print(dataframes[0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7fb40",
   "metadata": {},
   "source": [
    "## Checking the right date format was saved and adding year/month columns to match main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add year and month columns to each DataFrame in the list\n",
    "def add_year_month_columns(dataframes):\n",
    "    for i in range(len(dataframes)):\n",
    "        df = dataframes[i]\n",
    "        \n",
    "        # Extract the year and month from the 'Date' column\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        \n",
    "        # Replace the DataFrame in the list with the new columns added\n",
    "        dataframes[i] = df\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "# Apply the function to the list of DataFrames\n",
    "dataframes = add_year_month_columns(dataframes)\n",
    "\n",
    "# checking if transformation worked in the dataframes list:\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2f829",
   "metadata": {},
   "source": [
    "### Define chronologiav order for dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of DataFrames in the desired order\n",
    "dfs_in_order = [dataframes[3],dataframes[2],dataframes[4],dataframes[6],dataframes[5],dataframes[1],dataframes[0]]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "merged_df = pd.concat(dfs_in_order,ignore_index=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775441b6",
   "metadata": {},
   "source": [
    "from the info function displays the new data contains 152 observations across 8 columss from march 24 to september 24.\n",
    "the first colum shows dates in datetime format, followed by each category of coffee as well as the index values as floats. the added year and month number of each observation is in integer format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7df6bf",
   "metadata": {},
   "source": [
    "### Rename column names prior to merging both datasets\n",
    "\n",
    "This will enable to combine previous data from original dataset to have a bigger pool of observations to feed more data in the modeling part. Is expected the final dataset to combine data from feb/23 to sep/24\n",
    "\n",
    "- df1 = icip_data > contains the original dataset (Feb 2023 - Feb 2024)\n",
    "- df2 = merged_df > contains the new dataset (Mar 2024 - Sep 2024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = icip_data\n",
    "df2 = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the 'Date' column in both df1 and df2 is in datetime format\n",
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "df2['date'] = pd.to_datetime(df2['date'])\n",
    "\n",
    "# Rename the columns in df2 to match the structure of df1\n",
    "df2.columns = ['date', 'I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas', 'year', 'month']\n",
    "\n",
    "# Concatenate df1 and df2 into a single DataFrame\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Sort by the 'Date' column to ensure chronological order\n",
    "combined_df = combined_df.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a CSV file\n",
    "combined_df.to_csv('final_combined_data.csv', index=False)\n",
    "\n",
    "# Test: print the first few rows to verify the result\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e579854",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfee9b",
   "metadata": {},
   "source": [
    "The combined dataset on the correct stucture can help to make better explorations on the next sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52b5d1",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6613f4ac",
   "metadata": {},
   "source": [
    "### Check fo rmissing values and summary statisctis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c552177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "# Get summary statistics for numerical columns\n",
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7f577",
   "metadata": {},
   "source": [
    "### Plotting trends overtime to begin understanding how this new dataset is presented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47eeda7",
   "metadata": {},
   "source": [
    "## 2.1 Prices plots\n",
    "\n",
    "###  a. ICO Composite Indicator Price (I-CIP) is the main feature to be used for the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4779103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot I-CIP over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(combined_df['date'], combined_df['I-CIP'], label='I-CIP')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('I-CIP')\n",
    "plt.title('I-CIP Trend Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f81958",
   "metadata": {},
   "source": [
    "## b. Comparing the different categories over time:\n",
    "\n",
    "Each category has a different weight to calculate the final composite. **(get data on this)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56743c02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot other categories of coffee over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(combined_df['date'], combined_df['I-CIP'], label='I-CIP')\n",
    "plt.plot(combined_df['date'], combined_df['colombian_milds'], label='Colombian Milds')\n",
    "plt.plot(combined_df['date'], combined_df['other_milds'], label='Other Milds')\n",
    "plt.plot(combined_df['date'], combined_df['brazilian_nat'], label='Brazilian Naturals')\n",
    "plt.plot(combined_df['date'], combined_df['robustas'], label='Robustas')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Coffee Types Trend Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44f9a0",
   "metadata": {},
   "source": [
    "## c. Compare ICIP to each coffee category over time\n",
    "changing labels for date axis for easier visualisation (ie 2023-03 to MAR 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Define only the columns you want to plot (excluding the last two columns)\n",
    "columns_to_plot = ['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate over the selected columns and plot each one\n",
    "for column in columns_to_plot:\n",
    "    plt.plot(combined_df['date'], combined_df[column], label=column)\n",
    "\n",
    "# Customize x-axis to show months (use date format for better readability)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.title('Price Fluctuations of ICO Composite Indicator and Coffee Groups Over Time')\n",
    "plt.legend()\n",
    "\n",
    "# Format the x-axis labels to show the month name with better spacing\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # Shows every 3rd month\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "\n",
    "plt.xticks(rotation=45)  # Rotate for better readability\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d7822",
   "metadata": {},
   "source": [
    "### 2.2 Setting the date column as index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new variable for merged_df and reseting date as the index for building time series in later stages\n",
    "# Set 'date' column as index\n",
    "combined_df.set_index('date', inplace=True)\n",
    "\n",
    "#check output\n",
    "icip_df = combined_df\n",
    "icip_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01de80d",
   "metadata": {},
   "source": [
    "### 2.2.1 Checking the range of dataset: \n",
    "\n",
    "\n",
    "With the dates as indext we can check the range of the dataset: \n",
    "\n",
    "- 607 days however the data collected is at a frequency of BUSINESS DAYS, excluding weekends and holidays, which would account for the difference between the total days (607) and the number of observations (431)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking how many days are present in the dataset\n",
    "\n",
    "print(f'Dataframe contains prices between {icip_df.index.min()} {icip_df.index.max()}')\n",
    "print(f'Total Days = {icip_df.index.max() - icip_df.index.min()} days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a7b8c",
   "metadata": {},
   "source": [
    "### Once the date is set as index, is possible to measure the range an frequency of data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure the index is set at datetime \n",
    "icip_df.index = pd.to_datetime(icip_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the range, confirm the frequency of the index\n",
    "print(icip_df.index.freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5b3a2",
   "metadata": {},
   "source": [
    "A freq marked as 'None' makes python treat the date as irregular. Manually setting the frquency as Business days since the frequancy is not really defined.\n",
    "This can have a series of benefits:\n",
    "- Align  data with time-based operations.\n",
    "- Perform accurate rolling calculations and time series decomposition.\n",
    "- Handle missing data systematically.\n",
    "- Use advanced time series models and resampling.\n",
    "\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/version/0.16/timeseries.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75768777",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df = icip_df.asfreq('B')  # B stands for Business Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1958f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the range, confirm the frequency of the index\n",
    "print(icip_df.index.freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88962c4a",
   "metadata": {},
   "source": [
    "### 2.2.1\n",
    "\n",
    "### a. Checking monthly seasonality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month from the index\n",
    "\n",
    "## plot only for 2023 and plot a separate for 2024\n",
    "#are variations in price the same in both year????\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "icip_df['year'] = icip_df.index.year\n",
    "icip_df['month'] = icip_df.index.month_name().str[:3]  # This will give  the three-letter month abbreviation.\n",
    "\n",
    "# Draw Plot\n",
    "plt.figure(figsize=(12, 7), dpi=80)\n",
    "sns.boxplot(x='month', y='I-CIP', data=icip_df)\n",
    "\n",
    "# Set Title\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices of 2023\\n(The Seasonality)', fontsize=18)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694c2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f509a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by year\n",
    "df_2023 = icip_df[icip_df['year'] == 2023]\n",
    "df_2024 = icip_df[icip_df['year'] == 2024]\n",
    "\n",
    "# Plot for 2023\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='month', y='I-CIP', data=df_2023, order=['Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices in 2023\\n(The Seasonality)', fontsize=18)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('I-CIP Price (In US cents/lb)')\n",
    "plt.show()\n",
    "\n",
    "# Plot for 2024 (up to September)\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='month', y='I-CIP', data=df_2024, order=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep'])\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices in 2024\\n(The Seasonality)', fontsize=18)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('I-CIP Price (In US cents/lb)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e946343a",
   "metadata": {},
   "source": [
    "### Year-over-Year Monthly Comparison:\n",
    "\n",
    "This plot aims to compare the avarage of icip prices in 2023 and 2024, to highlight differences in each year.\n",
    "\n",
    "Overall, the year of 24 is represented by higher average, confirming the upward trend seen in line plots in item c. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly average comparison\n",
    "monthly_avg = icip_df.groupby(['year', 'month'])['I-CIP'].mean().unstack(level=0)\n",
    "monthly_avg.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Average Monthly I-CIP Prices in 2023 vs 2024')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average I-CIP Price (in US cents/lb)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20a0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc2e6f92",
   "metadata": {},
   "source": [
    "###  Monthly Average I-CIP Prices with Regional Harvest Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae978a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127e560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the month order to ensure chronological sorting\n",
    "month_order = list(calendar.month_abbr[1:])  # ['Jan', 'Feb', ..., 'Dec']\n",
    "icip_df['month'] = pd.Categorical(icip_df['month'], categories=month_order, ordered=True)\n",
    "\n",
    "# Group by both year and month to keep month names in chronological order\n",
    "monthly_avg_df = icip_df.groupby([icip_df.index.year, 'month'])['I-CIP'].mean().unstack(level=0)\n",
    "\n",
    "# Create the plot with annotations of harvest \n",
    "plt.figure(figsize=(14, 7))\n",
    "monthly_avg_df.plot(kind='bar', color=['skyblue', 'salmon'], ax=plt.gca())\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average I-CIP Price (in US cents/lb)')\n",
    "plt.title('Monthly Average I-CIP Prices by Year with Harvest Annotations')\n",
    "\n",
    "# Annotate months with regional harvests (approximately)\n",
    "harvest_annotations = {\n",
    "    'Jan': 'South America, Africa',\n",
    "    'Feb': 'South America, Africa',\n",
    "    'Mar': 'South America',\n",
    "    'Apr': 'Central America, South America',\n",
    "    'May': 'Asia',\n",
    "    'Jun': 'South America, Africa, Asia',\n",
    "    'Jul': 'Asia, Africa',\n",
    "    'Aug': 'Asia, Africa',\n",
    "    'Sep': 'Asia',\n",
    "    'Oct': 'South America, Africa, Asia',\n",
    "    'Nov': 'Central America, Africa',\n",
    "    'Dec': 'South America, Africa'\n",
    "}\n",
    "\n",
    "# Add annotations at 45-degree angle for readability\n",
    "for month_idx, (month, regions) in enumerate(harvest_annotations.items()):\n",
    "    plt.text(month_idx - 0.15, monthly_avg_df.loc[month].max() + 5, \n",
    "             regions, ha='center', rotation=45, color='black', fontsize=8)\n",
    "\n",
    "# Adjust x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Year\", loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# harverst dates extracted from \n",
    "#Source: https://coffeehunter.com/coffee-seasonality/ accessed on 30/10\n",
    "# https://www.fairmountaincoffee.com/category-s/102.htm accessedon 30/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc98a72",
   "metadata": {},
   "source": [
    "#### Heatmap of Monthly Price Averages \n",
    "\n",
    "The heatmat below aims to identify months where prices tend to dip or spike, then cross-reference with known harvest periods. The color intensity provides a quick overview of price levels each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741dfecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for monthly averages\n",
    "monthly_avg_df = icip_df.groupby([icip_df.index.year, icip_df.index.month])['I-CIP'].mean().unstack()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(monthly_avg_df, annot=True, cmap=\"YlGnBu\", fmt=\".1f\", linewidths=0.5)\n",
    "plt.title('Monthly I-CIP Price Averages (in US cents/lb)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c243f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a307c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281a40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123d4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43f79099",
   "metadata": {},
   "source": [
    "## d. Value distribution across categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0449bbe",
   "metadata": {},
   "source": [
    "###### plot monthy only by category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of the original DataFrame without 'year' and 'month' columns\n",
    "copy = icip_df.drop(columns=['year', 'month']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064a401",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define colors from the Set2 palette\n",
    "colors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854']\n",
    "\n",
    "# Create boxplot traces\n",
    "box_traces = []\n",
    "for i, column in enumerate(copy.columns):\n",
    "    box_trace = go.Box(y=copy[column], name=column, marker=dict(color=colors[i]))\n",
    "    box_traces.append(box_trace)\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(title='Boxplot by Group', yaxis=dict(title='Value'), xaxis=dict(title='Variable'))\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure(data=box_traces, layout=layout)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5599914",
   "metadata": {},
   "source": [
    "colombian_milds and brazilian_nat have similar median values but different spreads of data, suggesting that the two groups exhibit relatively stable prices.\n",
    "\n",
    "\n",
    "robustas has the largest range of prices, which could indicate greater market volatility or variability in this coffee category.\n",
    "\n",
    "\n",
    "I-CIP shows a balanced range, but it’s positioned lower than both colombian_milds and brazilian_nat, which are premium categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208fbd3",
   "metadata": {},
   "source": [
    "the last 2 columns were only added to facilite some of the montkly plots, ill copy the main data as a separate dataframe for more statistical measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90cf07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7177048",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a8cb9",
   "metadata": {},
   "source": [
    "### Comparing mean values betrween categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba61abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "copy.mean().plot(kind='bar', color='skyblue')\n",
    "plt.title('Mean Values of Groups')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Mean')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c84f05",
   "metadata": {},
   "source": [
    "## 2.2.2 Checking correlation across categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = copy.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e807ac2",
   "metadata": {},
   "source": [
    "\n",
    "ICO Composite Indicator Price (I-CIP) is calculated using a weighted average of four coffee groups: Colombian Milds, Other Milds, Brazilian Naturals, and Robustas, with each group contributing a specific weight to the calculation:\n",
    "\n",
    "- Colombian Milds: 12%\n",
    "- Other Milds: 21%\n",
    "- Brazilian Naturals: 30%\n",
    "- Robustas: 37%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea66f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(copy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c73be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20461938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weighted I-CIP\n",
    "copy['weighted_I-CIP'] = (0.12 * copy['colombian_milds'] +\n",
    "                                 0.21 * copy['other_milds'] +\n",
    "                                 0.30 * copy['brazilian_nat'] +\n",
    "                                 0.37 * copy['robustas'])\n",
    "\n",
    "# Compare weighted I-CIP with actual I-CIP\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(copy.index, copy['I-CIP'], label='Actual I-CIP')\n",
    "plt.plot(copy.index, copy['weighted_I-CIP'], label='Weighted I-CIP', linestyle='--')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('I-CIP')\n",
    "plt.title('Actual I-CIP vs Weighted I-CIP')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190816c0",
   "metadata": {},
   "source": [
    "#### Lag plots\n",
    "understanding the entropy of icip prices and how correlated they are\n",
    "\n",
    "the scatter plot bellow shows the relationship between observations and their lags.\n",
    "\"as the lag increases, the correlation between the time series and its lags generally decreases.\"\n",
    "\n",
    "Some sort of autocorrelation in the data is visible in lag 1, (t+1). A strong linear relationship indicates a high correlation between an observation and its immediate predecessor. a similar pattern is observed in lag2, with a few datapoints begining to get apart. Lags 3 and 4 are already more spreaded, meaning the correlation between values is also decreasing as the interval between lags grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f082ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "plt.rcParams.update({'ytick.left' : False, 'axes.titlepad':10})\n",
    "\n",
    "lp = icip_df['I-CIP']\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)\n",
    "for i, ax in enumerate(axes.flatten()[:4]):\n",
    "    lag_plot(lp, lag=i+1, ax=ax, c='firebrick')\n",
    "    ax.set_title('Lag ' + str(i+1))\n",
    "\n",
    "    \n",
    "fig.suptitle('Lag Plots of I-CIP prices \\n(Points get wide and scattered with increasing lag -> lesser correlation)\\n', y=1.15)    \n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of lags for 1 month\n",
    "number_of_lags = 21\n",
    "\n",
    "# Create subplots with 3 columns\n",
    "fig, axes = plt.subplots(nrows=7, ncols=3, figsize=(15, 20))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Generate a lag plot for each lag\n",
    "for i in range(1, number_of_lags + 1):\n",
    "    lag_plot(icip_df['I-CIP'], lag=i, ax=axes[i-1])\n",
    "    axes[i-1].set_title(f'Lag {i}')\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfc70e",
   "metadata": {},
   "source": [
    "### Rolling average / Rolling standard deviation\n",
    "\n",
    "\n",
    "Rolling Mean: The rolling mean is the average of the previous observation window, where the window consists of a series of values from the time series data. Computing the mean for each ordered window. This can significantly help minimize noise in time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc708917",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rolling Statistics at different periods\n",
    "window_sizes = [5, 21, 63]  # A week, a month, a quarter,  (approximately)\n",
    "data_rolling = icip_df['I-CIP']  \n",
    "\n",
    "for window in window_sizes:\n",
    "    rolling_mean = icip_df['I-CIP'].rolling(window=window).mean()\n",
    "    rolling_std = icip_df['I-CIP'].rolling(window=window).std()\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(icip_df['I-CIP'].index, icip_df['I-CIP'], label='Original')\n",
    "    plt.plot(rolling_mean.index, rolling_mean, label=f'Rolling Mean (window={window})')\n",
    "    plt.plot(rolling_std.index, rolling_std, label=f'Rolling Std Dev (window={window})')\n",
    "    plt.title(f'Rolling Mean and Standard Deviation (window size = {window})')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118a95a",
   "metadata": {},
   "source": [
    "### checking for missing dates for determine the right frequency\n",
    "\n",
    "\n",
    "from ealier sections, it was observed that there were 100 days missing from the 365 window of dates whithin the dataset. However,  since this dataset displays data from monday-friday (business days) it was \"assumed\" (in data science we cant make assumptions but still...) that those 'missing' dates were only referent to weekends and/or holidays. This code below extracs the exact dates missing from the entire range for business days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c51b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a date range for 366 days from the start of your data\n",
    "# Adjust the period accordingly if you have data spanning multiple years or a different time frame\n",
    "start_date = icip_df.index.min()\n",
    "end_date = icip_df.index.max()  \n",
    "\n",
    "\n",
    "# Generate a range of business days within this period\n",
    "business_days = pd.bdate_range(start=start_date, end=end_date)\n",
    "\n",
    "# Now compare the business_days with your DataFrame's index to find out missing dates\n",
    "missing_dates = business_days.difference(icip_df.index)\n",
    "\n",
    "print(f\"Total number of expected business days: {len(business_days)}\")\n",
    "print(f\"Total number of actual days in data: {icip_df.shape[0]}\")\n",
    "print(f\"Total number of missing dates: {len(missing_dates)}\")\n",
    "print(\"Missing dates are:\")\n",
    "print(missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abb5b9",
   "metadata": {},
   "source": [
    "The code above says there are no missing dates in the dataframe, however, it still shows there are a few NaN values, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41092b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "icip_df.info()\n",
    "print(icip_df.shape)\n",
    "print(icip_df.isnull().sum())\n",
    "icip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695439a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df = icip_df.isna()\n",
    "print(nan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows = icip_df.isna().any(axis=1)\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c92348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter rows with nan values\n",
    "nan_rows = icip_df[icip_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c77976",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(icip_df.isna(), cbar=False, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dates with missing data\n",
    "nan_rows.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ba323",
   "metadata": {},
   "source": [
    "## IDENTIFIED MISSING DATES \n",
    "## INTERPOLATE FOR THE TIMESERIES\n",
    "\n",
    "MODELS CANT HANDLE MISSING DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd75f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997503bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064769ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print out results in customised manner\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "def kpss_test(timeseries):\n",
    "    print ('Results of KPSS Test:')\n",
    "    kpsstest = kpss(timeseries, regression='c', nlags=\"auto\")\n",
    "    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','#Lags Used'])\n",
    "    for key,value in kpsstest[3].items():\n",
    "        kpss_output['Critical Value (%s)'%key] = value\n",
    "    print (kpss_output)\n",
    "\n",
    "# Call the function and run the test\n",
    "\n",
    "kpss_test(icip_df['I-CIP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70ad47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9a56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2267ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal decompositions with different periods.\n",
    "\n",
    "periods = [63, 21, 5]  # Quartely, Monthly and Weekly (considering business days)\n",
    "# Function to generate the plots for all periods.\n",
    "for period in periods:\n",
    "    decompositions = seasonal_decompose(icip_df['I-CIP'], model='additive', period=period)\n",
    "\n",
    "    # Plotting the components of the decomposition\n",
    "    plt.rcParams.update({'figure.figsize': (8,8)})\n",
    "    print(f\"Seasonal Decomposition with Period = {period}\")\n",
    "    decompositions.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2cbee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79f07ea",
   "metadata": {},
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f7812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
