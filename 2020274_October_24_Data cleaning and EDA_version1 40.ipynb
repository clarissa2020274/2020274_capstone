{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f1b685",
   "metadata": {},
   "source": [
    "## MSc Data Analytics - Capstone Project\n",
    "\n",
    "#### Predictive Analysis in the Coffee Market: Using Deep Learning to predict coffee prices.\n",
    "Student id: 2020274 Clarissa Cardoso\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This notebook aims to analyse historical coffee price indices and develop a predictive model for future price trends in the green coffee market. The focus is on using data from the ICO (International Coffee Organization), particularly the composite prices indec (I-CIP), that combines prices of Colombian Milds, Other Milds, Brazilian Naturals, and Robustas.\n",
    "\n",
    "### Dataset:\n",
    "The dataset used in this analysis consists of historical coffee price data, with daily observations for business days. Prices are expressed in cents of USD per lb. and stated on a\n",
    "differential basis to the New York and London futures exchanges (https://icocoffee.org/wp-content/uploads/2023/01/icc-105-17-r1e-rules-indicator-prices.pdf)\n",
    "\n",
    "The data utilized in this project is sourced from the International Coffee Organization's (ICO) Public Market Information (https://ico.org/resources/public-market-information/), which provides the I-CIP values free of charge.\n",
    "\n",
    "For the early stages of the experimentation, 1 year worth of data was available to collect, from 01Feb23 to 29Feb24, which is present on a separate notebook (2020274_capstone_EDA_Models 2.ipynb). In this notebook, recent data from March to September 2024 were added to expand insights and feed more datapoints to modelling stage. \n",
    "\n",
    "\n",
    "### Objectives:\n",
    "1. Clean and preprocess the dataset for missing values and inconsistencies.\n",
    "2. Explore the time-series behavior of coffee prices through visualizations.\n",
    "3. Implement various forecasting models to predict future price trends, including traditional statistical models (e.g., ARIMA/Sarima) and deep learning algorithms (e.g., LSTM neural networks).\n",
    "4. Compare model performance using key metrics (e.g., RMSE, MAE).\n",
    "\n",
    "\n",
    "### Expected Outcome:\n",
    "By the end of this notebook, we will identify the best forecasting model for coffee prices and present actionable insights based on the findings.\n",
    "\n",
    "        Forecasting: generate forecasts for future I-CIP values using the best-performing model(s) and visualize the results to facilitate interpretation and decision-making.\n",
    "- 1 day\n",
    "- 5 days = 1 week\n",
    "- 21 days = 1 month\n",
    "\n",
    "\n",
    "(- 63 days = 3 months (1 quarter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b5168",
   "metadata": {},
   "source": [
    "### Importing relevant libraries for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd396c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd #dataframes \n",
    "import numpy as np #linear algebra\n",
    "import seaborn as sns #visualization\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "import scipy.stats as stats #statistical resources\n",
    "\n",
    "import matplotlib.pyplot as plt #visualisation \n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.model_selection import train_test_split # importing function to split the data training and test.\n",
    "from sklearn.preprocessing import MinMaxScaler # Import the MinMaxScaler module from sklearn.preprocessing library\n",
    "from sklearn.linear_model import LinearRegression # importing to performe linear regression. \n",
    "from sklearn.metrics import make_scorer, r2_score # Importing from Metrics module\n",
    "from sklearn.preprocessing import StandardScaler # standardize the data\n",
    "from sklearn import metrics # Metrics module from scikit-learn\n",
    "from sklearn.model_selection import GridSearchCV # importing for hyperparameter tunning\n",
    "from sklearn.metrics import mean_squared_error # importing mse\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential #last update in python causing dead kernel wehn importing keras functions?\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c486ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "## cheking if keras/tensorflow are correclty installed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e8f09",
   "metadata": {},
   "source": [
    "# 1. Load data\n",
    "\n",
    "For the early stages of this experimentation present on the first notebook (Models2_copy), 1 year worth of data was available to collect, from 01Feb23 to 29Feb24.\n",
    "\n",
    "This section will review the original dataset compiled with data from feb 23 to feb 24.\n",
    "\n",
    "\n",
    "A few thingsobserved when importing the raw files from the data source: \n",
    "\n",
    "- Column mismatch: Assuming all files have the same column names and order. This could lead to errors when merging DataFrames with different structures. \n",
    "\n",
    "The data for each month is published separetely. Originally the 4 first months had different colum labels for the same data 'ICO Composite' , while the following months was simpler version as 'I-CIP'. For that diverson it was not possible to simply merge all dataframes into one. Dta cleaning/manipulation techniques of renamimbg and reorganising them chronologically were adopeted to reach the final dataset for the first year of data alocated in the 'icip_data' below. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331a3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the CSV file \n",
    "icip_data = pd.read_csv(\"icip_df.csv\")\n",
    "\n",
    "# View the first 5 rows\n",
    "icip_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41abde",
   "metadata": {},
   "source": [
    "Since then, ICO has released additional months that will be included in the main dataframe, considering the timeframe from march to september 2024 as a way to feed more data to the models with the expectation it could improve the results. These seven new files will be sorted by chronological order and have the same labels as the main one above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42756de2",
   "metadata": {},
   "source": [
    "### Importing  additional data from March/24 to September/24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# List all the files in the folder\n",
    "os.listdir(\"icip_24\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e7b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8486d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create for loop to import csv files from the folder with less comands.\n",
    "\n",
    "# create an empty list to store dfs\n",
    "dataframes = []\n",
    "\n",
    "# path to folder where csv files are (in this case same directory)\n",
    "folder_path = \"icip_24\"\n",
    "\n",
    "\n",
    "# to import CSV starting from the third row, skipping the first two\n",
    "def import_csv(filepath):\n",
    "    return pd.read_csv(filepath, skiprows=2)\n",
    "\n",
    "# Iterate through files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):  # Only consider CSV files\n",
    "        file_path = os.path.join(folder_path, file)  # Construct the full file path\n",
    "        dataframes.append(import_csv(file_path))  # Read CSV and append to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ef932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad605ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the lenght of the directory, how many files exist in the new folder\n",
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1135e0d1",
   "metadata": {},
   "source": [
    "Chcking the heading of the files to undertand how features are allocated in this first stage before combining the new 7 months to main dataframe\n",
    "\n",
    "The same issue appears with the heading names. So this time around it was decided to ignore the first 2 rows to avoid the unnamed labels and only import the actual data \n",
    "\n",
    ">Unnamed: 0\tUnnamed: 1\tColombian\tUnnamed: 3\tBrazilian\tUnnamed: 5\n",
    "\n",
    "\n",
    ">0\tNaN\tI-CIP\tNaN\tOther Milds\tNaN\tRobusta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if order of files correspond with the directory list, testing if loop is working\n",
    "dataframes[5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387462db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dataframes)\n",
    "#list of all dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8ce2b",
   "metadata": {},
   "source": [
    "To continue the project is necessary to make 2 adjustments in the second directory:\n",
    "- change the date format from \" 06-Jun\" to '%Y-%m-%d' format and apply this to all files in the \"Unnamed: 0\" collum which corresponds to date. This will enable a more smooth combination of the 2 dfs once all dates mantain the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: print the first DataFrame to check if the transformation worked\n",
    "print(dataframes[5].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac339ca",
   "metadata": {},
   "source": [
    "The code below will use the month mapping to rearranje data in the first colum from corresponding to dates.\n",
    "- first it will change the format from '**01-Jul**' to **''%Y-%m-%d''** through all months in the list of dataframes.\n",
    "- then will replace the dataframes with the correct format and set as 'datetime' type. \n",
    "- print the correct year for all dataframes and show the correct label for 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd76423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform the 'Unnamed: 0' date column for each DataFrame in the list and reorder columns\n",
    "def transform_date(dataframes, year):\n",
    "    month_mapping = {\n",
    "        'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "        'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "        'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "    }\n",
    "    \n",
    "    # Iterate over each DataFrame in the list\n",
    "    for i in range(len(dataframes)):\n",
    "        df = dataframes[i]\n",
    "        \n",
    "        # Print the columns to inspect if 'Unnamed: 0' exists or if the name is different\n",
    "        print(f\"Columns in DataFrame {i}: {df.columns}\")\n",
    "        \n",
    "        # Check if 'Unnamed: 0' exists, otherwise handle the column name differently\n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            # Apply the transformation to the 'Unnamed: 0' column to create full date strings\n",
    "            df['date'] = df['Unnamed: 0'].apply(\n",
    "                lambda x: '-'.join([str(year), month_mapping[x.split('-')[1]], x.split('-')[0]])\n",
    "            )\n",
    "            \n",
    "            # Convert the 'Date' column to datetime format\n",
    "            df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "            \n",
    "            # Drop the original 'Unnamed: 0' column\n",
    "            df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "            \n",
    "            # Reorder columns to place 'Date' first\n",
    "            columns = ['date'] + [col for col in df.columns if col != 'date']\n",
    "            dataframes[i] = df[columns]  # Replace the DataFrame with the reordered one\n",
    "        else:\n",
    "            print(f\"'Unnamed: 0' column not found in DataFrame {i}\")\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "# Apply the function to the list of DataFrames\n",
    "dataframes = transform_date(dataframes, 2024)\n",
    "\n",
    "# Test: print the first DataFrame to check if the column reordering worked\n",
    "print(dataframes[0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7fb40",
   "metadata": {},
   "source": [
    "## Splitting the date column to match main dataset\n",
    "\n",
    "This function is mainly to add the last 2 columns matching the original dataset which contains the year and m,onth of each date. This can also be used for some of exploratory plots in next sectiosn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add year and month columns to each DataFrame in the list\n",
    "def add_year_month_columns(dataframes):\n",
    "    for i in range(len(dataframes)):\n",
    "        df = dataframes[i]\n",
    "        \n",
    "        # Extract the year and month from the 'Date' column\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        \n",
    "        # Replace the DataFrame in the list with the new columns added\n",
    "        dataframes[i] = df\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "# Apply the function to the list of DataFrames\n",
    "dataframes = add_year_month_columns(dataframes)\n",
    "\n",
    "# checking if transformation worked in the dataframes list:\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2f829",
   "metadata": {},
   "source": [
    "### Define chronologic order for dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of DataFrames in the desired order\n",
    "dfs_in_order = [dataframes[3],dataframes[2],dataframes[4],dataframes[6],dataframes[5],dataframes[1],dataframes[0]]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "merged_df = pd.concat(dfs_in_order,ignore_index=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775441b6",
   "metadata": {},
   "source": [
    "from the info function displays the new data contains 152 observations across 8 columss from march 24 to september 24.\n",
    "the first colum shows dates in datetime format, followed by each category of coffee as well as the index values as floats. the added year and month number of each observation is in integer format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7df6bf",
   "metadata": {},
   "source": [
    "### Rename column names prior to merging both datasets\n",
    "\n",
    "This will enable to combine previous data from original dataset to have a bigger pool of observations to feed more data in the modeling part. Is expected the final dataset to combine data from feb/23 to sep/24\n",
    "\n",
    "- df1 = icip_data > contains the original dataset (Feb 2023 - Feb 2024)\n",
    "- df2 = merged_df > contains the new dataset (Mar 2024 - Sep 2024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = icip_data\n",
    "df2 = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the 'Date' column in both df1 and df2 is in datetime format\n",
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "df2['date'] = pd.to_datetime(df2['date'])\n",
    "\n",
    "# Rename the columns in df2 to match the structure of df1\n",
    "df2.columns = ['date', 'I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas', 'year', 'month']\n",
    "\n",
    "# Concatenate df1 and df2 into a single DataFrame\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Sort by the 'Date' column to ensure chronological order\n",
    "combined_df = combined_df.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a CSV file\n",
    "#combined_df.to_csv('final_combined_data.csv', index=False)\n",
    "\n",
    "# Test: print the first few rows to verify the result\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e579854",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfee9b",
   "metadata": {},
   "source": [
    "The combined dataset on the correct stucture can help to make better explorations on the next sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52b5d1",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "\n",
    "This section aims to see how the data is presented, such as basic statistics and its distribution over time.  They can help to identify average levels, variability, and the range of values \n",
    "(large deviations might suggest volatility, which could be important for modeling in later stages of experimentation)\n",
    "\n",
    "\n",
    "### 2.1 Summary statistics and checking for missing values\n",
    "\n",
    "From the *info* fucntion we get 431 rows of numerical data, with the first colum showing values in datetime type followed by the ICO composite prices and each category of coffee used for the index. The last 2 columsn are the ones added earlier with the month and year of each data point. \n",
    "\n",
    "This analysis will help to interpret basic price dynamics before diving into more complex forecasting techniques, such as SaARIMA and LSTMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic info\n",
    "\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics for numeric columns \n",
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "print(combined_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39fc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "\n",
    "print(combined_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f491c5",
   "metadata": {},
   "source": [
    "## 2.2 Trends over time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7f577",
   "metadata": {},
   "source": [
    "### Plotting trends overtime to begin understanding how this new dataset is presented\n",
    "\n",
    "This also helps in highlighting any major shifts or trends which are quite pronounced in this dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f81958",
   "metadata": {},
   "source": [
    "### a. Comparing the different categories over time:\n",
    "\n",
    "Each category has a different weight to calculate the final composite. **(get data on this)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56743c02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot other categories of coffee over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(combined_df['date'], combined_df['I-CIP'], label='I-CIP')\n",
    "plt.plot(combined_df['date'], combined_df['colombian_milds'], label='Colombian Milds')\n",
    "plt.plot(combined_df['date'], combined_df['other_milds'], label='Other Milds')\n",
    "plt.plot(combined_df['date'], combined_df['brazilian_nat'], label='Brazilian Naturals')\n",
    "plt.plot(combined_df['date'], combined_df['robustas'], label='Robustas')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (US cents/lb)')\n",
    "plt.title('Coffee Types Trend Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44f9a0",
   "metadata": {},
   "source": [
    "\n",
    "changing labels for date axis for easier visualisation (ie from 2023-03 to MAR 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Define only the columns you want to plot (excluding the last two columns)\n",
    "columns_to_plot = ['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate over the selected columns and plot each one\n",
    "for column in columns_to_plot:\n",
    "    plt.plot(combined_df['date'], combined_df[column], label=column)\n",
    "\n",
    "# Customize x-axis to show months (use date format for better readability)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Price (US cents/lb)')\n",
    "plt.title('Price Fluctuations of ICO Composite Indicator and Coffee Groups Over Time')\n",
    "plt.legend()\n",
    "\n",
    "# Format the x-axis labels to show the month name with better spacing\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # Shows every 3rd month\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "\n",
    "plt.xticks(rotation=45)  # Rotate for better readability\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd619dc",
   "metadata": {},
   "source": [
    "A few things to consider fgrom this main plot:\n",
    "\n",
    "\n",
    "The plot above illustrates the price fluctuations of the ICO Composite Indicator (I-CIP) and four key coffee categories—Colombian Milds, Other Milds, Brazilian Naturals, and Robustas—over a period from Febuary 2023 to September 2024. \n",
    "\n",
    "all categories exhibit a general upward trend, reflecting an overall increase in coffee prices. \n",
    "\n",
    "Colombian Milds consistently maintains the highest price, followed by Other Milds, with Brazilian Naturals and Robustas being comparatively lower in price.\n",
    "\n",
    "The I-CIP line in blue serves as a composite indicator, providing an average of these categories. Notable price peaks appear around March 2023 and mid-2024, suggesting potential seasonal or market-related impacts on prices. Toward the end of the timeline, there is a significant price surge across all categories, which may be attributed to factors such as supply chain disruptions or increased global demand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d7822",
   "metadata": {},
   "source": [
    "###  Setting the date column as index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new variable for merged_df and reseting date as the index for building time series in later stages\n",
    "# Set 'date' column as index\n",
    "combined_df.set_index('date', inplace=True)\n",
    "\n",
    "#check output\n",
    "icip_df = combined_df\n",
    "icip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01de80d",
   "metadata": {},
   "source": [
    "###  b. Checking the range of dataset: \n",
    "\n",
    "\n",
    "With the dates as indext we can check the range of the dataset: \n",
    "\n",
    "- 607 days however the data collected is at a frequency of BUSINESS DAYS, excluding weekends and holidays, which would account for the difference between the total days (607) and the number of observations (431)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking how many days are present in the dataset\n",
    "\n",
    "print(f'Dataframe contains prices between {icip_df.index.min()} {icip_df.index.max()}')\n",
    "print(f'Total Days = {icip_df.index.max() - icip_df.index.min()} days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a7b8c",
   "metadata": {},
   "source": [
    "### Once the date is set as index, is possible to measure the range an frequency of data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure the index is set at datetime \n",
    "icip_df.index = pd.to_datetime(icip_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the range, confirm the frequency of the index\n",
    "print(icip_df.index.freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5b3a2",
   "metadata": {},
   "source": [
    "A freq marked as 'None' makes python treat the date as irregular. Manually setting the frquency as Business days since the frequancy is not really defined.\n",
    "This can have a series of benefits:\n",
    "- Align  data with time-based operations.\n",
    "- Perform accurate rolling calculations and time series decomposition.\n",
    "- Handle missing data systematically.\n",
    "- Use advanced time series models and resampling.\n",
    "\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/version/0.16/timeseries.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75768777",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df = icip_df.asfreq('B')  # B stands for Business Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1958f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the range, confirm the frequency of the index\n",
    "print(icip_df.index.freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7365a",
   "metadata": {},
   "source": [
    "After setting the freqeuncy as busines days, the index automatically added 3 more empty rows to the dataset.\n",
    "index shows **434** entries while non null count is **431**. That means data imputation is required (which will be addressed in further sections for data preparation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99eac29",
   "metadata": {},
   "source": [
    "### c. Value distribution across categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b26ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define colors from the Set2 palette\n",
    "colors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854']\n",
    "\n",
    "# Specify the columns you want to include in the boxplot\n",
    "selected_columns = ['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']  \n",
    "\n",
    "# Create boxplot traces\n",
    "box_traces = []\n",
    "for i, column in enumerate(selected_columns):\n",
    "    color = colors[i % len(colors)] \n",
    "    box_trace = go.Box(y=icip_df[column], name=column, marker=dict(color=color))\n",
    "    box_traces.append(box_trace)\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(title='Boxplot by Column', yaxis=dict(title='Value'), xaxis=dict(title='Variable'))\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure(data=box_traces, layout=layout)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619911d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3eddf46",
   "metadata": {},
   "source": [
    "## d. Mean values of each category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bar plot of mean values for each column\n",
    "plt.figure(figsize=(10, 6))\n",
    "icip_df[selected_columns].mean().plot(kind='bar', color='skyblue')\n",
    "plt.title('Mean Values of Each Variable')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Mean')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378aa53",
   "metadata": {},
   "source": [
    "## e.  Correlation Matrix for Coffee Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix for Coffee Prices\n",
    "correlation_matrix = icip_df[['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.matshow(correlation_matrix, cmap='coolwarm', fignum=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation Matrix of Coffee Prices\")\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78792ae",
   "metadata": {},
   "source": [
    "###### plot monthy only by category "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ccdc2",
   "metadata": {},
   "source": [
    ">>The matrix highlights that colombian_milds and other_milds are closely related and likely to be primary influencers on the I-CIP index. In contrast, robustas exhibits more independence from other types. These relationships provide a foundation for selecting variables in a forecasting model, where incorporating highly correlated categories might improve predictive accuracy.<<"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47eeda7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  2.2.1 ICO Composite Indicator Price (I-CIP) \n",
    "\n",
    "I-CIP is the main feature to be used for this analysis. \n",
    " \n",
    " \n",
    "There’s a clear upward trend in the I-CIP index from early 2023 to late 2024. This suggests that coffee prices, as represented by I-CIP, have generally increased over this period.\n",
    "\n",
    "Initial Fluctuations (Early to Mid-2023): In the first part of the timeline (early to mid-2023), the I-CIP prices exhibit noticeable fluctuations, with a few peaks and troughs. This suggests some instability in the market, with prices rising and falling frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4779103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot I-CIP over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(icip_df['I-CIP'], label='I-CIP')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('I-CIP')\n",
    "plt.title('I-CIP Trend Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d00f2",
   "metadata": {},
   "source": [
    "looking closer at the gap in thi line plot:\n",
    "between dec 23 and jan 24 there seems to have a missing value there\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b68538",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70b3cdeb",
   "metadata": {},
   "source": [
    "### 2.2.1\n",
    "\n",
    "### a. Checking monthly seasonality\n",
    "\n",
    "This plot shows the seasonalityvariation in the composite prices over the years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334f285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b113c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract year and month from the index\n",
    "\n",
    "## plot only for 2023 and plot a separate for 2024\n",
    "#are variations in price the same in both year????\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "icip_df['year'] = icip_df.index.year\n",
    "icip_df['month'] = icip_df.index.month_name().str[:3]  # This will give  the three-letter month abbreviation.\n",
    "\n",
    "# Draw Plot\n",
    "plt.figure(figsize=(12, 7), dpi=80)\n",
    "sns.boxplot(x='month', y='I-CIP', data=icip_df)\n",
    "\n",
    "# Set Title\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices of 2023/2024\\n(The Seasonality)', fontsize=18)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37db924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "icip_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f810aca",
   "metadata": {},
   "source": [
    "##  b. Separating values montly per each year\n",
    "\n",
    "Re-arranging the order for yearly plots since the data is not complete, since we have for 2023: Feb to Dec and 2024: Jan to Sep, the labels are done manually below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca20b20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter data by year\n",
    "df_2023 = icip_df[icip_df['year'] == 2023]\n",
    "df_2024 = icip_df[icip_df['year'] == 2024]\n",
    "\n",
    "# Plot for 2023\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='month', y='I-CIP', data=df_2023, order=['Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices in 2023\\n(The Seasonality)', fontsize=18)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('I-CIP Price (In US cents/lb)')\n",
    "plt.show()\n",
    "\n",
    "# Plot for 2024 (up to September)\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='month', y='I-CIP', data=df_2024, order=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep'])\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices in 2024\\n(The Seasonality)', fontsize=18)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('I-CIP Price (In US cents/lb)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0e699",
   "metadata": {},
   "source": [
    "## 2.3 Data Cleaning: Checking for Missing Dates \n",
    "\n",
    "for determine the right frequency\n",
    "\n",
    "\n",
    " The output of the first code below shows exacly how many days contains in the dataset, based on the indexes. while the second one will say how many of those total days are actually the business days containing data for the analisis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking how many days are present in the dataset\n",
    "\n",
    "print(f'Dataframe contains prices between {icip_df.index.min()} {icip_df.index.max()}')\n",
    "print(f'Total Days = {icip_df.index.max() - icip_df.index.min()} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51354e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a date range for 366 days from the start of the data\n",
    "\n",
    "start_date = icip_df.index.min()\n",
    "end_date = icip_df.index.max()  \n",
    "\n",
    "\n",
    "# Generate a range of business days within this period\n",
    "business_days = pd.bdate_range(start=start_date, end=end_date)\n",
    "\n",
    "# Now compare the business_days with icip_df index to find out missing dates\n",
    "missing_dates = business_days.difference(icip_df.index)\n",
    "\n",
    "print(f\"Total number of expected business days: {len(business_days)}\")\n",
    "print(f\"Total number of actual days in data: {icip_df.shape[0]}\")\n",
    "print(f\"Total number of missing dates: {len(missing_dates)}\")\n",
    "print(\"Missing dates are:\")\n",
    "print(missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e66a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d42e9949",
   "metadata": {},
   "source": [
    "The code above says there are no missing dates in the dataframe, however, it still shows there are a few NaN values. This is confirmed below with the *is null* code that shows a total of 3 missing values across all columns:  out of the 434 entries computed by the datetime index, ranging from 2023-02-01 to 2024-09-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ad06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of Nan values\n",
    "#icip_df.info()\n",
    "print(icip_df.shape)\n",
    "print(icip_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc5276",
   "metadata": {},
   "source": [
    "**These different methods all confirm there are missing values present in the combined dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is any missing values will come back TRUE\n",
    "icip_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25424a",
   "metadata": {},
   "source": [
    "Booleans will say True for any missing valies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df = icip_df.isna()\n",
    "print(nan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25731ed",
   "metadata": {},
   "source": [
    "Separate dataframe isolating the dates with True/False booleans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fb662",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows = icip_df.isna().any(axis=1)\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe02898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter rows with nan values\n",
    "nan_rows = icip_df[icip_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63a673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "525636d3",
   "metadata": {},
   "source": [
    "**Plot figure to highlight missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08afe3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(icip_df.isna(), cbar=False, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59e045",
   "metadata": {},
   "source": [
    "**EXTRACT MISSING DATES FROM THE INDEX**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f756f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dates with missing data\n",
    "nan_rows.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e9f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed98f74",
   "metadata": {},
   "source": [
    "## Once the  MISSING DATES are identified\n",
    "## 2.4 DATA IMPUTATION METHODS\n",
    "\n",
    "MODELS CANT HANDLE MISSING DATA \n",
    "\n",
    "**Missing dates**  \n",
    "- **'2023-12-25'**: December 25th: Christmas Day\n",
    "- **'2023-12-26'**: December 26th: Often observed as Boxing Day or a Christmas holiday extension\n",
    "- **'2024-01-01'**: New Year's Day\n",
    "\n",
    "From the early eda, it was observed no missing values per se, however, for a timeseries analysis, the date in the index must be following the correct sequence (\"order?\"). it's the main characteristic of modeling this type of data. So eventhough there were no missing values (all dates had values) the date range was incomplete. \n",
    "\n",
    "**\n",
    "Another insight from this is that the composite price is published by the ICO even in regular holidays? \n",
    "The price composite is agregated values from US and EU (germany+france), this could be an indicative of why there are only 3 dates not in their database. \n",
    "\n",
    "For this project, lets add these three dates by chacking the moving average and the foward fill methods.\n",
    "There is also an alternatite to model the holiday effect on trend/seasonality of the data by including special events or dummy variables (but for the purpose of this experimentation, it wont be necessary because only 3 dates are a relatively small number of holidays to be considered in modeling. \n",
    "\n",
    "## a. DAta interpolation \n",
    "\n",
    "This section will compare 3 different methods for interpolation of missing values in the ICIP dataset\n",
    "\n",
    "- foward fill\n",
    "- backward fill \n",
    "- linear interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb668585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906840b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of the original DataFrame without 'year' and 'month' columns\n",
    "copy = icip_df.drop(columns=['year', 'month']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd19c05",
   "metadata": {},
   "source": [
    "the last 2 columns were only added to facilite some of the montkly plots, ill copy the main data as a separate dataframe for more statistical measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Fill\n",
    "df_ffill = copy.ffill()\n",
    "\n",
    "# Backward Fill\n",
    "df_bfill = copy.bfill()\n",
    "\n",
    "# Linear Interpolation \n",
    "df_interpolate = copy.interpolate()\n",
    "\n",
    "# Get the indices of rows with NaN values in the original DataFrame\n",
    "nan_indices = copy[copy.isna().any(axis=1)].index\n",
    "\n",
    "# compare the results for the `I-CIP` column at the NaN rows\n",
    "print(\"Forward Fill:\")\n",
    "print(df_ffill['I-CIP'].loc[nan_indices])\n",
    "\n",
    "print(\"\\nBackward Fill:\")\n",
    "print(df_bfill['I-CIP'].loc[nan_indices])\n",
    "\n",
    "print(\"\\nInterpolation:\")\n",
    "print(df_interpolate['I-CIP'].loc[nan_indices])\n",
    "\n",
    "## https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b8c23",
   "metadata": {},
   "source": [
    "### b.  Visual comparison of data imputation methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b0956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for comparison to avoid modifying the original 'copy'\n",
    "compare_df = copy.copy()\n",
    "\n",
    "# Apply imputation methods to create comparison columns\n",
    "compare_df['price_forward_fill'] = copy['I-CIP'].ffill()\n",
    "compare_df['price_backward_fill'] = copy['I-CIP'].bfill()\n",
    "compare_df['price_interpolate'] = copy['I-CIP'].interpolate()\n",
    "\n",
    "# Create subplots for visual comparison of imputation methods\n",
    "fig, axes = plt.subplots(4, 1, sharex=True, figsize=(12, 14))\n",
    "\n",
    "# Set date format for x-axis\n",
    "axes[3].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "axes[3].xaxis.set_major_locator(mdates.MonthLocator(interval=2))  # Adjust interval as needed\n",
    "axes[3].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "# Plot the original data with missing points on each subplot for comparison\n",
    "axes[0].plot(compare_df.index, compare_df['I-CIP'], label='Original Data (with NaNs)', color='blue', linestyle='-', marker='o')\n",
    "axes[0].set_title('Original Data with Missing Points')\n",
    "axes[0].legend()\n",
    "\n",
    "# Forward Fill - plot both original and forward-filled data\n",
    "axes[1].plot(compare_df.index, compare_df['I-CIP'], label='Original Data (with NaNs)', color='blue', linestyle='-', marker='o')\n",
    "axes[1].plot(compare_df.index, compare_df['price_forward_fill'], label='Forward Fill', color='orange', linestyle='--', marker='x')\n",
    "axes[1].set_title('Forward Fill Comparison')\n",
    "axes[1].legend()\n",
    "\n",
    "# Backward Fill - plot both original and backward-filled data\n",
    "axes[2].plot(compare_df.index, compare_df['I-CIP'], label='Original Data (with NaNs)', color='blue', linestyle='-', marker='o')\n",
    "axes[2].plot(compare_df.index, compare_df['price_backward_fill'], label='Backward Fill', color='red', linestyle='--', marker='+')\n",
    "axes[2].set_title('Backward Fill Comparison')\n",
    "axes[2].legend()\n",
    "\n",
    "# Linear Interpolation - plot both original and interpolated data\n",
    "axes[3].plot(compare_df.index, compare_df['I-CIP'], label='Original Data (with NaNs)', color='blue', linestyle='-', marker='o')\n",
    "axes[3].plot(compare_df.index, compare_df['price_interpolate'], label='Linear Interpolation', color='green', linestyle='--', marker='s')\n",
    "axes[3].set_title('Linear Interpolation Comparison')\n",
    "axes[3].legend()\n",
    "\n",
    "# Set labels and rotate date labels\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].set_ylabel('Price')\n",
    "plt.setp(axes[3].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9800a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "992e8757",
   "metadata": {},
   "source": [
    "Linear interpolation is typically the most appropriate for time series data because it provides a gradual transition between values, which better mimics natural fluctuations in price indices.\n",
    "Forward and Backward Fill are simpler methods and may be suitable if you believe that missing values should hold steady from a previous or upcoming value, but they can introduce unrealistic flat segments.\n",
    "\n",
    "\n",
    "https://medium.com/@aaabulkhair/data-imputation-demystified-time-series-data-69bc9c798cb7\n",
    "\n",
    "https://365datascience.com/tutorials/time-series-analysis-tutorials/pre-process-time-series-data/\n",
    "\n",
    "### c. Adding linear interpolation to main dataset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60770cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply linear interpolation to fill missing values in the DataFrame\n",
    "copy_interpolated = copy.interpolate(method='linear')\n",
    "\n",
    "# Verify that there are no remaining NaN values\n",
    "print(copy_interpolated.isna().sum())\n",
    "\n",
    "# Display a sample of the DataFrame to check the interpolation results\n",
    "copy_interpolated.head()\n",
    "#print(copy_interpolated.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f48ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c269fe0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695736e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7af54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6858b3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aa5a95b",
   "metadata": {},
   "source": [
    "## 2.5 EDA with interpolated dataset\n",
    "\n",
    "## a. boxplot with the interpolated data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57402b0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "copy_interpolated['year'] = copy_interpolated.index.year\n",
    "copy_interpolated['month'] = copy_interpolated.index.month_name().str[:3]  # This will give  the three-letter month abbreviation.\n",
    "\n",
    "# Draw Plot\n",
    "plt.figure(figsize=(12, 7), dpi=80)\n",
    "sns.boxplot(x='month', y='I-CIP', data=copy_interpolated)\n",
    "\n",
    "# Set Title\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices of 2023/2024\\n(The Seasonality)', fontsize=18)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff87728",
   "metadata": {},
   "source": [
    "### b.Pairplot across categories \n",
    "\n",
    "The darker points (higher I-CIP values) are more prevalent in 2024, indicating an increase in the I-CIP index, which aligns with the upward trend seen in the scatter plots across coffee types.\n",
    "\n",
    "There’s a high correlation among colombian_milds, other_milds, brazilian_nat, and robustas, which indicates that changes in the price of one type are closely mirrored by changes in others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(copy_interpolated, hue='I-CIP', diag_kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318d1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8dbb4d3",
   "metadata": {},
   "source": [
    "## 2.5.1 Montly prices (boxplot) per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by year\n",
    "df_23 = copy_interpolated[copy_interpolated['year'] == 2023]\n",
    "df_24 = copy_interpolated[copy_interpolated['year'] == 2024]\n",
    "\n",
    "# Plot for 2023\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='month', y='I-CIP', data=df_23, order=['Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices in 2023\\n(The Seasonality)', fontsize=18)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('I-CIP Price (In US cents/lb)')\n",
    "plt.show()\n",
    "\n",
    "# Plot for 2024 (up to September)\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='month', y='I-CIP', data=df_24, order=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep'])\n",
    "plt.title('Month-wise Box Plot of I-CIP Prices in 2024\\n(The Seasonality)', fontsize=18)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('I-CIP Price (In US cents/lb)')\n",
    "plt.show()\n",
    "\n",
    "copy_interpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc75eb",
   "metadata": {},
   "source": [
    "#### Plot month-wise price distribution for each cetegory\n",
    "\n",
    "This structure makes it easy to compare the seasonal trends across different coffee types.\n",
    "\n",
    "\n",
    "    Colombian Milds consistently have higher prices than the other categories, followed by Other Milds and Brazilian Naturals. Robustas remain at the lowest price range. This order aligns with market expectations, as Colombian and Other Milds are typically *considered higher-quality coffee* (ref) types and therefore command higher prices. Across all categories, prices tend to be lower in the early months (February to May) and then gradually increase throughout the middle of the year, peaking around September.For most categories, there is a noticeable drop in prices from October to January, which could reflect seasonal demand changes or other external market factors affecting coffee prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of categories to plot\n",
    "categories = ['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Create subplots: 1 row for each category\n",
    "fig, axes = plt.subplots(num_categories, 1, figsize=(12, 14), sharex=True)\n",
    "\n",
    "# Plot each category in a separate subplot\n",
    "for i, category in enumerate(categories):\n",
    "    sns.boxplot(x='month', y=category, data=copy_interpolated, ax=axes[i])\n",
    "    axes[i].set_title(f'Month-wise Box Plot of {category} Prices (2023/2024)', fontsize=12)\n",
    "    axes[i].set_ylabel('Price (US cents/lb)')\n",
    "    \n",
    "# Set common x-axis label and rotate month labels for readability\n",
    "axes[-1].set_xlabel('Month')\n",
    "for ax in axes:\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Adjust layout for clarity\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce9fe54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8631e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20894030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040a2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9c94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24903bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febc748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d8d50a",
   "metadata": {},
   "source": [
    "2.5.1\n",
    "\n",
    "## a. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where prices are outliers for each month\n",
    "# \n",
    "q1 = icip_df.groupby('month')['I-CIP'].quantile(0.25)\n",
    "q3 = icip_df.groupby('month')['I-CIP'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Define outliers as points outside 1.5 * IQR\n",
    "outliers = icip_df.apply(lambda x: (x['I-CIP'] < q1[x['month']] - 1.5 * iqr[x['month']]) |\n",
    "                               (x['I-CIP'] > q3[x['month']] + 1.5 * iqr[x['month']]), axis=1)\n",
    "\n",
    "# Display or analyze the outliers\n",
    "outlier_data = icip_df[outliers]\n",
    "outlier_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f444264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the I-CIP prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(copy_interpolated.index, copy_interpolated['I-CIP'], label='I-CIP Prices', color='blue')\n",
    "plt.scatter(outlier_data.index, outlier_data['I-CIP'], color='red', label='Outliers')  # Highlight outliers\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title('I-CIP Prices with Outliers Highlighted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('I-CIP Price (in US cents/lb)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9df1ffc",
   "metadata": {},
   "source": [
    "b. Explore Correlations with Other Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc22d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select the relevant columns (e.g., I-CIP and other coffee categories) for the outliers\n",
    "outlier_correlation = outlier_data[['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']]\n",
    "\n",
    "# Step 2: Calculate correlations\n",
    "outlier_corr = outlier_correlation.corr()\n",
    "print(outlier_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554c300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e946343a",
   "metadata": {},
   "source": [
    "### c. Year-over-Year Monthly Comparison:\n",
    "\n",
    "### Monthly average\n",
    "\n",
    "This plot aims to compare the avarage of icip prices in 2023 and 2024, to highlight differences in each year.\n",
    "\n",
    "Overall, the year of 24 is represented by higher average, confirming the upward trend seen in line plots in item c.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae978a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the month order to ensure chronological sorting\n",
    "month_order = list(calendar.month_abbr[1:])  # ['Jan', 'Feb', ..., 'Dec']\n",
    "icip_df['month'] = pd.Categorical(icip_df['month'], categories=month_order, ordered=True)\n",
    "\n",
    "# Monthly average comparison\n",
    "monthly_avg = icip_df.groupby(['year', 'month'])['I-CIP'].mean().unstack(level=0)\n",
    "monthly_avg.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Average Monthly I-CIP Prices in 2023 vs 2024')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average I-CIP Price (in US cents/lb)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3962408",
   "metadata": {},
   "source": [
    "months appear in alphabetical order instead of chronological? >>> **adjust\\!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly Average Plot\n",
    "monthly_avg = icip_df['I-CIP'].resample('M').mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(monthly_avg, label=\"Monthly Average I-CIP\")\n",
    "plt.title(\"Monthly Average I-CIP Prices\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"I-CIP Price (In US cents/lb)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e6f92",
   "metadata": {},
   "source": [
    "### d.  Monthly Average I-CIP Prices with Regional Harvest Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127e560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the month order to ensure chronological sorting\n",
    "#month_order = list(calendar.month_abbr[1:])  # ['Jan', 'Feb', ..., 'Dec']\n",
    "#icip_df['month'] = pd.Categorical(icip_df['month'], categories=month_order, ordered=True)\n",
    "\n",
    "# Group by both year and month to keep month names in chronological order\n",
    "monthly_avg_df = icip_df.groupby([icip_df.index.year, 'month'])['I-CIP'].mean().unstack(level=0)\n",
    "\n",
    "# Create the plot with annotations of harvest \n",
    "plt.figure(figsize=(14, 7))\n",
    "monthly_avg_df.plot(kind='bar', color=['skyblue', 'salmon'], ax=plt.gca())\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average I-CIP Price (in US cents/lb)')\n",
    "plt.title('Monthly Average I-CIP Prices by Year with Harvest Annotations')\n",
    "\n",
    "# Annotate months with regional harvests (approximately)\n",
    "harvest_annotations = {\n",
    "    'Jan': 'South America, Africa',\n",
    "    'Feb': 'South America, Africa',\n",
    "    'Mar': 'South America',\n",
    "    'Apr': 'Central America, South America',\n",
    "    'May': 'Asia',\n",
    "    'Jun': 'South America, Africa, Asia',\n",
    "    'Jul': 'Asia, Africa',\n",
    "    'Aug': 'Asia, Africa',\n",
    "    'Sep': 'Asia',\n",
    "    'Oct': 'South America, Africa, Asia',\n",
    "    'Nov': 'Central America, Africa',\n",
    "    'Dec': 'South America, Africa'\n",
    "}\n",
    "\n",
    "# Add annotations at 45-degree angle for readability\n",
    "for month_idx, (month, regions) in enumerate(harvest_annotations.items()):\n",
    "    plt.text(month_idx - 0.15, monthly_avg_df.loc[month].max() + 5, \n",
    "             regions, ha='center', rotation=45, color='black', fontsize=8)\n",
    "\n",
    "# Adjust x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Year\", loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# harverst dates extracted from \n",
    "#Source: https://coffeehunter.com/coffee-seasonality/ accessed on 30/10\n",
    "# https://www.fairmountaincoffee.com/category-s/102.htm accessedon 30/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc98a72",
   "metadata": {},
   "source": [
    "#### Heatmap of Monthly Price Averages \n",
    "\n",
    "The heatmat below aims to identify months where prices tend to dip or spike, then cross-reference with known harvest periods. The color intensity provides a quick overview of price levels each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741dfecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for monthly averages\n",
    "monthly_avg_df = icip_df.groupby([icip_df.index.year, icip_df.index.month])['I-CIP'].mean().unstack()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(monthly_avg_df, annot=True, cmap=\"YlGnBu\", fmt=\".1f\", linewidths=0.5)\n",
    "plt.title('Monthly I-CIP Price Averages (in US cents/lb)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = copy_interpolated\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281a40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7215551d",
   "metadata": {},
   "source": [
    "## e. Monthly variation (%) of for each category\n",
    "### ICIP and its components \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data to monthly frequency and calculate percentage change\n",
    "monthly_variations = new_df[['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']].resample('M').ffill().pct_change() * 100\n",
    "\n",
    "# Set up the figure and subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 12), sharex=True)\n",
    "fig.suptitle(\"Monthly Variation (%) of Coffee Prices by Category\", fontsize=16)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# List of categories to plot\n",
    "categories = ['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']\n",
    "\n",
    "# Plot each category's monthly variation in its own subplot\n",
    "for i, category in enumerate(categories):\n",
    "    axes[i].plot(monthly_variations.index, monthly_variations[category], label=f'{category} Monthly Variation (%)')\n",
    "    axes[i].axhline(0, color='gray', linestyle='--')  # Add a horizontal line at 0\n",
    "    axes[i].set_title(f\"{category} Monthly Variation\")\n",
    "    axes[i].set_ylabel(\"Monthly Variation (%)\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(len(categories), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.xlabel(\"Date\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Leave space for the main title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d07cef",
   "metadata": {},
   "source": [
    "## 2.5.2 Volatility\n",
    "## a. Weekly variations\n",
    "\n",
    "\n",
    "Resampling the data to weekly frequency and calculate the percentage change from one week to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weekly percentage change (variation) for each category\n",
    "weekly_variations = new_df[['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']].resample('W').ffill().pct_change() * 100\n",
    "\n",
    "# Plot each category's weekly variation in separate subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 12), sharex=True)\n",
    "fig.suptitle(\"Weekly Variation (%) of Coffee Prices by Category\", fontsize=16)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# List of categories to plot\n",
    "categories = ['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']\n",
    "\n",
    "# Plot each category's weekly variation\n",
    "for i, category in enumerate(categories):\n",
    "    axes[i].plot(weekly_variations.index, weekly_variations[category], label=f'{category} Weekly Variation (%)')\n",
    "    axes[i].axhline(0, color='gray', linestyle='--')  # Add a horizontal line at 0\n",
    "    axes[i].set_title(f\"{category} Weekly Variation\")\n",
    "    axes[i].set_ylabel(\"Weekly Variation (%)\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(len(categories), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.xlabel(\"Date\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Leave space for the main title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7798c8ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb4c46ab",
   "metadata": {},
   "source": [
    "## b. Daily variations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily percentage change (variation) for each category\n",
    "daily_variations = new_df[['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']].pct_change() * 100\n",
    "\n",
    "# Plot each category's daily variation in separate subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 12), sharex=True)\n",
    "fig.suptitle(\"Daily Variation (%) of Coffee Prices by Category\", fontsize=16)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each category's daily variation\n",
    "for i, category in enumerate(categories):\n",
    "    axes[i].plot(daily_variations.index, daily_variations[category], label=f'{category} Daily Variation (%)')\n",
    "    axes[i].axhline(0, color='gray', linestyle='--')  # Add a horizontal line at 0\n",
    "    axes[i].set_title(f\"{category} Daily Variation\")\n",
    "    axes[i].set_ylabel(\"Daily Variation (%)\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(len(categories), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.xlabel(\"Date\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Leave space for the main title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ebc69",
   "metadata": {},
   "source": [
    "## c. Highlight the high volatility periods\n",
    "\n",
    "## Weekly and daily\n",
    "\n",
    "Based on industry standards, as cited by Lordemann et al(2021) \n",
    "\n",
    "- Weekly Volatility Threshold: Flag weeks with variations above ±5%.\n",
    "Weekly Threshold: Weekly changes greater than ±5% are often used in commodity markets to flag significant shifts, particularly for goods with high seasonality or sensitivity to external events.\n",
    "\n",
    "- Daily Volatility Threshold: Flag days with variations above ±3%.\n",
    "In financial markets, daily price changes greater than ±3% are often considered high volatility. Since coffee prices can be sensitive to market and environmental factors, this threshold could be reasonable.\n",
    "\n",
    "Source: https://repositorio.cepal.org/server/api/core/bitstreams/ca31a1a5-c246-49e4-81e0-be62a16f3c26/content\n",
    "\n",
    "#### identify treshholds\n",
    "As an a dditional justification for the tresholds above, the code below shows a percentile based approach to compare the industry standards (+-3% and +-5%) to the actual extreme variations in the ICIP data, which falls close to the benchmark numbers. \n",
    "With a daily treshold of 2.72% and weekly of 6.53%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f47642",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculate percentiles for daily and weekly variations\n",
    "daily_threshold = daily_variations['I-CIP'].quantile(0.95)\n",
    "weekly_threshold = weekly_variations['I-CIP'].quantile(0.95)\n",
    "\n",
    "print(f\"95th Percentile-Based Daily Threshold: ±{daily_threshold:.2f}%\")\n",
    "print(f\"95th Percentile-Based Weekly Threshold: ±{weekly_threshold:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d83e6a",
   "metadata": {},
   "source": [
    "#### Define most volatile days and weeks based on threshold calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7933fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds based on daily/weekly variations calculated above\n",
    "weekly_threshold = 6.53  # Weekly variation above +/- 6.53% considered volatile\n",
    "daily_threshold = 2.72   # Daily variation above +/-2.72 considered volatile \n",
    "\n",
    "# Find volatile weeks\n",
    "volatile_weeks = weekly_variations[(weekly_variations > weekly_threshold) | (weekly_variations < -weekly_threshold)].dropna(how='all')\n",
    "print(\"Volatile Weeks:\")\n",
    "print(volatile_weeks)\n",
    "\n",
    "# Find volatile days\n",
    "volatile_days = daily_variations[(daily_variations > daily_threshold) | (daily_variations < -daily_threshold)].dropna(how='all')\n",
    "print(\"Volatile Days:\")\n",
    "print(volatile_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1e6d9",
   "metadata": {},
   "source": [
    "## Visualise Volatile periods\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb754311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot daily variation with volatile days highlighted\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_variations.index, daily_variations['I-CIP'], label='I-CIP Daily Variation')\n",
    "plt.scatter(volatile_days.index, volatile_days['I-CIP'], color='red', label='Volatile Days', marker='o')\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.title(\"I-CIP Daily Variation with Volatile Days Highlighted\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Variation (%)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac6241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79240ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for weekly variations\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(weekly_variations.index, weekly_variations['I-CIP'], label='I-CIP Weekly Variation')\n",
    "plt.scatter(volatile_weeks.index, volatile_weeks['I-CIP'], color='red', label='Volatile Weeks', marker='o')\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.title(\"I-CIP Weekly Variation with Volatile Weeks Highlighted\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Weekly Variation (%)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b91841",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_variations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a8e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling standard deviation (e.g., 7-day) for each category\n",
    "rolling_volatility = daily_variations['I-CIP'].rolling(window=7).std()\n",
    "\n",
    "# Plot the rolling volatility\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rolling_volatility.index, rolling_volatility, label=\"7-Day Rolling Volatility\", color='blue')\n",
    "plt.axhline(y=rolling_volatility.mean(), color='red', linestyle='--', label=\"Mean Volatility\")\n",
    "plt.title(\"7-Day Rolling Volatility for I-CIP\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Rolling Volatility (%)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install calmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ac62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calmap\n",
    "\n",
    "# Resample to daily absolute values of I-CIP volatility for visualization\n",
    "daily_volatility_icip = daily_variations['I-CIP'].abs()\n",
    "\n",
    "# Plot calendar heatmap for daily volatility\n",
    "plt.figure(figsize=(14, 8))\n",
    "calmap.calendarplot(daily_volatility_icip, cmap='Reds', fillcolor='lightgrey', linewidth=0.1)\n",
    "plt.title(\"Calendar Heatmap of Daily Volatility for I-CIP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9583bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d8499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf1341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d8a7209",
   "metadata": {},
   "source": [
    " Rolling Volatility (Standard Deviation)\n",
    "Another approach for pinpointing volatility through a sliding window using a rolling standard deviation.\n",
    "The code belows attemp tto do it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6614263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 4-week rolling standard deviation for each category\n",
    "weekly_rolling_volatility = weekly_variations.rolling(window=4).std()\n",
    "\n",
    "# Identify periods where rolling volatility exceeds a chosen threshold\n",
    "high_weekly_volatility = weekly_rolling_volatility[weekly_rolling_volatility > weekly_threshold]\n",
    "print(\"High Weekly Volatility Periods:\")\n",
    "print(high_weekly_volatility.dropna(how='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7f9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 7-day rolling standard deviation for each category\n",
    "daily_rolling_volatility = daily_variations.rolling(window=7).std()\n",
    "\n",
    "# Identify periods where rolling volatility exceeds a chosen threshold\n",
    "high_daily_volatility = daily_rolling_volatility[daily_rolling_volatility > daily_threshold]\n",
    "print(\"High Daily Volatility Periods:\")\n",
    "print(high_daily_volatility.dropna(how='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b670f2b",
   "metadata": {},
   "source": [
    "## d. Checking periods of high volatility across all variables?\n",
    "\n",
    "## Daily variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29523de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your volatility threshold for daily variations\n",
    "daily_threshold = 2.72\n",
    "\n",
    "# Find volatile days for each category\n",
    "daily_volatile_days = daily_variations[(daily_variations > daily_threshold) | (daily_variations < -daily_threshold)]\n",
    "\n",
    "# Set up subplots for daily variations\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 12), sharex=True)\n",
    "fig.suptitle(\"Daily Variation (%) of Coffee Prices by Category with Volatile Days Highlighted\", fontsize=16)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each category's daily variation with volatile days highlighted\n",
    "for i, category in enumerate(categories):\n",
    "    axes[i].plot(daily_variations.index, daily_variations[category], label=f'{category} Daily Variation (%)')\n",
    "    axes[i].scatter(daily_volatile_days.index, daily_volatile_days[category], color='red', label='Volatile Days', marker='o')\n",
    "    axes[i].axhline(0, color='gray', linestyle='--')\n",
    "    axes[i].set_title(f\"{category} Daily Variation\")\n",
    "    axes[i].set_ylabel(\"Daily Variation (%)\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# Hide any unused subplots if there are extra spots\n",
    "for j in range(len(categories), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.xlabel(\"Date\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e03b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95c042e0",
   "metadata": {},
   "source": [
    "## Weekly variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cb80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming categories is the list of your column names, like:\n",
    "categories = ['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']\n",
    "\n",
    "# Define volatility threshold for weekly variations\n",
    "weekly_threshold = 6.5\n",
    "\n",
    "# Find volatile weeks for each category\n",
    "weekly_volatile_weeks = weekly_variations[(weekly_variations > weekly_threshold) | (weekly_variations < -weekly_threshold)]\n",
    "\n",
    "# Set up subplots for weekly variations\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 12), sharex=True)\n",
    "fig.suptitle(\"Weekly Variation (%) of Coffee Prices by Category with Volatile Weeks Highlighted\", fontsize=16)\n",
    "\n",
    "# Flatten axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each category's weekly variation with volatile weeks highlighted\n",
    "for i, category in enumerate(categories):\n",
    "    axes[i].plot(weekly_variations.index, weekly_variations[category], label=f'{category} Weekly Variation (%)')\n",
    "    axes[i].scatter(weekly_volatile_weeks.index, weekly_volatile_weeks[category], color='red', label='Volatile Weeks', marker='o')\n",
    "    axes[i].axhline(0, color='gray', linestyle='--')\n",
    "    axes[i].set_title(f\"{category} Weekly Variation\")\n",
    "    axes[i].set_ylabel(\"Weekly Variation (%)\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# Hide any unused subplots if there are extra spots\n",
    "for j in range(len(categories), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.xlabel(\"Date\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9b755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c9ef9511",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Load a world shapefile dataset (such as Natural Earth)\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Define crop years as dictionary mapping countries to their crop start months\n",
    "crop_years = {\n",
    "    \"April\": [\"Angola\", \"Indonesia\", \"Paraguay\", \"Bolivia\", \"Madagascar\", \"Peru\", \"Brazil\", \"Malawi\", \"Rwanda\", \"Burundi\", \"Papua New Guinea\", \"Zimbabwe\", \"Ecuador\"],\n",
    "    \"July\": [\"Congo\", \"Haiti\", \"United Republic of Tanzania\", \"Cuba\", \"Philippines\", \"Zambia\", \"Dominican Republic\"],\n",
    "    \"October\": [\"Benin\", \"Ghana\", \"Nigeria\", \"Cameroon\", \"Guatemala\", \"Panama\", \"Central African Republic\", \"Guinea\", \"Sierra Leone\", \"Colombia\", \"Honduras\", \"Sri Lanka\", \"Costa Rica\", \"India\", \"Thailand\", \"Cote d'Ivoire\", \"Jamaica\", \"Togo\", \"Democratic Republic of the Congo\", \"Kenya\", \"Trinidad & Tobago\", \"El Salvador\", \"Liberia\", \"Uganda\", \"Equatorial Guinea\", \"Mexico\", \"Venezuela\", \"Ethiopia\", \"Nicaragua\", \"Vietnam\", \"Gabon\"]\n",
    "}\n",
    "\n",
    "# Add a new column to the world GeoDataFrame for crop year\n",
    "world[\"crop_year\"] = world[\"name\"].apply(lambda x: \"April\" if x in crop_years[\"April\"] else \"July\" if x in crop_years[\"July\"] else \"October\" if x in crop_years[\"October\"] else \"Other\")\n",
    "\n",
    "# Plot the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "world.plot(column=\"crop_year\", cmap=\"Set2\", legend=True, ax=ax)\n",
    "plt.title(\"Coffee Producing Countries and Their Crop Year Start Dates\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5379585c",
   "metadata": {},
   "source": [
    "### Distribution of volatile Months and Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47c386",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add 'Month' and 'Weekday' columns to the DataFrame\n",
    "daily_variations['Month'] = daily_variations.index.month\n",
    "daily_variations['Weekday'] = daily_variations.index.day_name()\n",
    "\n",
    "# Box plot by month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Month', y='I-CIP', data=daily_variations, color='lightblue')\n",
    "plt.title(\"Monthly Distribution of Daily Volatility for I-CIP\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Daily Variation (%)\")\n",
    "plt.show()\n",
    "\n",
    "# Box plot by day of the week\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Weekday', y='I-CIP', data=daily_variations, color='lightgreen', order=[\n",
    "            'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "plt.title(\"Weekly Distribution of Daily Volatility for I-CIP\")\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "plt.ylabel(\"Daily Variation (%)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_variations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sns.pairplot(daily_variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63eef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a43e56d",
   "metadata": {},
   "source": [
    "## 2.6 Distribuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de44ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915014cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing some of the aesthetics... adding in a kernel desnity estimate:\n",
    "ax = sns.histplot(new_df['I-CIP'], kde=True, color ='green')\n",
    "ax.set(xlabel='i-cip',\n",
    "       ylabel='Frequency',\n",
    "       title =\"ICO Composite Prices  Histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee14ed4",
   "metadata": {},
   "source": [
    "From the histogram above, we can see the data doensnt show a normal distribution of values, the graph below a kde is added to the plot to make it even clearer. The Bimodal distribution indicates the presence of two distinct groups within the dataset, each with its own central tendency. (two populations in the sample)\n",
    "\n",
    "\n",
    "This often suggests that this dataset may be influenced by two different underlying processes or conditions. In terms of comodity markets, for example, a distribution like this one might reflect different market dynamics or consumption patterns across two separate seasons or market segments. (**find ref**) \n",
    "\n",
    "The next step is to consider whether these modes represent meaningful segments that require additional analysis or modeling strategies. By fitting a Gaussian Mixture Model or exploring the data's skewness and kurtosis, we can dive deeper into understanding the characteristics of these subgroups, which can give more insight son how to proceed to tailoring the forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6d7ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create histogram\n",
    "fig = px.histogram(new_df['I-CIP'], x='I-CIP', nbins=30) #changing the number of bins makes the curve clearer, 10, 20 were also used\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='I-CIP Histogram',\n",
    "    xaxis_title='Price',\n",
    "    yaxis_title='Frequency'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d26250",
   "metadata": {},
   "source": [
    "From the histogram above, we can see the data doensnt show a normal distribution of values, the graph below a kde is added to the plot to make it even clearer. The Bimodal distribution indicates the presence of two distinct groups within the dataset, each with its own central tendency. (two populations in the sample)\n",
    "\n",
    "\n",
    "This often suggests that this dataset may be influenced by two different underlying processes or conditions. In terms of comodity markets, for example, a distribution like this one might reflect different market dynamics or consumption patterns across two separate seasons or market segments.\n",
    "The next step is to consider whether these modes represent meaningful segments that require additional analysis or modeling strategies. By fitting a Gaussian Mixture Model or exploring the data's skewness and kurtosis, we can dive deeper into understanding the characteristics of these subgroups, which can give more insight son how to proceed to tailoring the forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1659737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with two subplots to check for distribution\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot histogram of icip prices in the first subplot\n",
    "axs[0].hist(copy['I-CIP'], bins=10)\n",
    "axs[0].set_title('Histogram of I-CIP values')\n",
    "axs[0].set_xlabel('Value')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot probability plot of prices in the second subplot\n",
    "stats.probplot(copy['I-CIP'], dist='norm', plot=axs[1])\n",
    "axs[1].set_title('Probability Plot of Values')\n",
    "\n",
    "# Adjust the layout to avoid overlapping titles and labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea735da",
   "metadata": {},
   "source": [
    "the data shows some characteristics of a normal distribution but with potential signs of bimodality and the presence of outliers or heavy tails. This can often happen in real-world data where multiple factors contribute to the observed values, which can lead to more complex distributions deviating from the ideally normal dist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33fb84e",
   "metadata": {},
   "source": [
    "### checking skewness of data\n",
    "\n",
    "from the plot below we see the data is presenting a bimodal distribuition, which indicates the non normality of values. \n",
    "\n",
    "The lack of simmetry in distribution (tail extending to the right side) is confirmed by printing the actual skeweness measure which is 0.889. \n",
    "\n",
    "Another statistical measure used was the kurtosis to identify potential outliers in the dataset, represented by the flatness of the tail at a 0.30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb219d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Visual Inspection\n",
    "plt.hist(new_df['I-CIP'], bins=30, alpha=0.5, color='blue', density=True)\n",
    "new_df['I-CIP'].plot(kind='density', color='green')\n",
    "plt.title('Histogram and Density Plot')\n",
    "plt.show()\n",
    "\n",
    "# Check skewness and kurtosis\n",
    "data_skewness = skew(new_df['I-CIP'])\n",
    "data_kurtosis = kurtosis(new_df['I-CIP'])\n",
    "print(f\"Skewness: {data_skewness}\")\n",
    "print(f\"Kurtosis: {data_kurtosis}\")\n",
    "\n",
    "# Fitting a Gaussian Mixture Model >>> cant fit this model when there are Nan values in the datatset\n",
    "gmm = GaussianMixture(n_components=2, random_state=0).fit(np.expand_dims(new_df['I-CIP'], 1))\n",
    "gmm_scores = gmm.score_samples(np.expand_dims(new_df['I-CIP'], 1))\n",
    "\n",
    "print(f\"GMM Converged: {gmm.converged_}\") # Check if the algorithm has converged\n",
    "print(f\"GMM Weights: {gmm.weights_}\") # Gives the weights of the two components\n",
    "\n",
    "# Plotting the GMM results\n",
    "plt.hist(new_df['I-CIP'], bins=30, alpha=0.5, color='blue', density=True)\n",
    "plt.plot(np.linspace(min(new_df['I-CIP']), max(new_df['I-CIP']), len(gmm_scores)), np.exp(gmm_scores), color='red')\n",
    "plt.title('Histogram with GMM-estimated Density')\n",
    "plt.show()\n",
    "\n",
    "#Source: https://builtin.com/articles/gaussian-mixture-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e17bb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1307f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3067f5b2",
   "metadata": {},
   "source": [
    "\n",
    "**ValueError: Input X contains NaN.** \n",
    "GaussianMixture does not accept missing values encoded as NaN natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa590261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e239d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f16362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc8222a5",
   "metadata": {},
   "source": [
    "### Weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e807ac2",
   "metadata": {},
   "source": [
    "\n",
    "ICO Composite Indicator Price (I-CIP) is calculated using a weighted average of four coffee groups: Colombian Milds, Other Milds, Brazilian Naturals, and Robustas, with each group contributing a specific weight to the calculation:\n",
    "\n",
    "- Colombian Milds: 12%\n",
    "- Other Milds: 21%\n",
    "- Brazilian Naturals: 30%\n",
    "- Robustas: 37%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea66f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c73be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20461938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weighted I-CIP\n",
    "new_df['weighted_I-CIP'] = (0.12 * new_df['colombian_milds'] +\n",
    "                                 0.21 * new_df['other_milds'] +\n",
    "                                 0.30 * new_df['brazilian_nat'] +\n",
    "                                 0.37 * new_df['robustas'])\n",
    "\n",
    "# Compare weighted I-CIP with actual I-CIP\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(new_df.index, copy_interpolated['I-CIP'], label='Actual I-CIP')\n",
    "plt.plot(new_df.index, copy_interpolated['weighted_I-CIP'], label='Weighted I-CIP', linestyle='--')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('I-CIP')\n",
    "plt.title('Actual I-CIP vs Weighted I-CIP')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190816c0",
   "metadata": {},
   "source": [
    "#### Lag plots for I-CIP values\n",
    "understanding the entropy of icip prices and how correlated they are\n",
    "\n",
    "the scatter plot bellow shows the relationship between observations and their lags.\n",
    "\"as the lag increases, the correlation between the time series and its lags generally decreases.\"\n",
    "\n",
    "Some sort of autocorrelation in the data is visible in lag 1, (t+1). A strong linear relationship indicates a high correlation between an observation and its immediate predecessor. a similar pattern is observed in lag2, with a few datapoints begining to get apart. Lags 3 and 4 are already more spreaded, meaning the correlation between values is also decreasing as the interval between lags grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f082ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "plt.rcParams.update({'ytick.left' : False, 'axes.titlepad':10})\n",
    "\n",
    "lp = new_df['I-CIP']\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)\n",
    "for i, ax in enumerate(axes.flatten()[:4]):\n",
    "    lag_plot(lp, lag=i+1, ax=ax, c='firebrick')\n",
    "    ax.set_title('Lag ' + str(i+1))\n",
    "\n",
    "    \n",
    "fig.suptitle('Lag Plots of I-CIP prices \\n(Points get wide and scattered with increasing lag -> lesser correlation)\\n', y=1.15)    \n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29257d7",
   "metadata": {},
   "source": [
    "The lag plots show the relationship between the current value of the I-CIP prices \n",
    "𝑦\n",
    "(\n",
    "𝑡\n",
    ")\n",
    "y(t) and its values at different time lags \n",
    "𝑦\n",
    "(\n",
    "𝑡\n",
    "+\n",
    "𝑘\n",
    ")\n",
    "y(t+k) for \n",
    "𝑘\n",
    "=\n",
    "1\n",
    ",\n",
    "2\n",
    ",\n",
    "3\n",
    ",\n",
    "k=1,2,3, and \n",
    "4\n",
    "4. Here’s an analysis of each lag plot:\n",
    "\n",
    "Lag 1:\n",
    "\n",
    "The points are tightly clustered along a diagonal line, indicating a strong positive correlation between \n",
    "𝑦\n",
    "(\n",
    "𝑡\n",
    ")\n",
    "y(t) and \n",
    "𝑦\n",
    "(\n",
    "𝑡\n",
    "+\n",
    "1\n",
    ")\n",
    "y(t+1).\n",
    "This suggests that there is a strong dependency between consecutive observations, meaning today’s price has a strong relationship with tomorrow’s price.\n",
    " the lag plots indicate that the I-CIP series has a strong immediate dependency (Lag 1) that diminishes quickly, supporting a model structure with low-order lags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of lags for 1 month\n",
    "number_of_lags = 21\n",
    "\n",
    "# Create subplots with 3 columns\n",
    "fig, axes = plt.subplots(nrows=7, ncols=3, figsize=(15, 20))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Generate a lag plot for each lag\n",
    "for i in range(1, number_of_lags + 1):\n",
    "    lag_plot(icip_df['I-CIP'], lag=i, ax=axes[i-1])\n",
    "    axes[i-1].set_title(f'Lag {i}')\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfc70e",
   "metadata": {},
   "source": [
    "### Rolling average / Rolling standard deviation\n",
    "\n",
    "\n",
    "Rolling Mean: The rolling mean is the average of the previous observation window, where the window consists of a series of values from the time series data. Computing the mean for each ordered window. This can significantly help minimize noise in time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc708917",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rolling Statistics at different periods\n",
    "window_sizes = [5, 21, 63]  # A week, a month, a quarter,  (approximately)\n",
    "data_rolling = new_df['I-CIP']  \n",
    "\n",
    "for window in window_sizes:\n",
    "    rolling_mean = new_df['I-CIP'].rolling(window=window).mean()\n",
    "    rolling_std = new_df['I-CIP'].rolling(window=window).std()\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(new_df['I-CIP'].index, new_df['I-CIP'], label='Original')\n",
    "    plt.plot(rolling_mean.index, rolling_mean, label=f'Rolling Mean (window={window})')\n",
    "    plt.plot(rolling_std.index, rolling_std, label=f'Rolling Std Dev (window={window})')\n",
    "    plt.title(f'Rolling Mean and Standard Deviation (window size = {window})')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d15706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7a401a9",
   "metadata": {},
   "source": [
    "## 2.7 Checking for Stationarity\n",
    "\n",
    "### Hypotesis testing: \n",
    "\n",
    "### Augmented Dickey-Fuller test\n",
    "\n",
    "ADF test is conducted with the following assumptions:\n",
    "\n",
    "- Null Hypothesis (HO): Series is non-stationary, or series has a unit root.\n",
    "- Alternate Hypothesis(HA): Series is stationary, or series has no unit root.\n",
    "If the null hypothesis is failed to be rejected, this test may provide evidence that the series is non-stationary. \n",
    "(https://www.analyticsvidhya.com/blog/2021/06/statistical-tests-to-check-stationarity-in-time-series-part-1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997503bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test\n",
    "# Function to print out results\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(timeseries):\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "    \n",
    "    \n",
    "# Call the function and run the test\n",
    "\n",
    "adf_test(new_df['I-CIP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ad5c85",
   "metadata": {},
   "source": [
    "The results of the ADF test indicate that the I-CIP series is non-stationary, as the standard to be considered is a  p value bewlow 0.05\n",
    "\n",
    "**p-value: 0.99** – p-value is quite high which means  we fail to reject the null hypothesis of the ADF test since is fallsabove the significance level. \n",
    "\n",
    "This suggests that the series has a unit root => **non-stationary**.\n",
    "\n",
    " [The code for both tests were extracted from Kumar (2023)\n",
    "https://www.analyticsvidhya.com/blog/2021/06/statistical-tests-to-check-stationarity-in-time-series-part-1/]\n",
    "\n",
    "\n",
    "Although the adf might be more comon for hypotesis testing, the kpps usualy acts as a complementary evidence for this section. \n",
    "\n",
    "\n",
    "### KPPS test for Stationarity\n",
    "\n",
    "The interpretation of the KPSS test is the opposite to the Dickey-Fuller test:\n",
    "\n",
    "If the p-value is less than the significance level *(p < 0.05)*, then we reject the null hypothesis, indicating that the time series is not trend-stationary.\n",
    "If the p-value is greater than the significance level, we fail to reject the null hypothesis, suggesting that the series may be trend-stationary.\n",
    "\n",
    "\n",
    "the **p-value = 0.01** is less than 0.05, indicating statistical significance. Therefore, we reject the null hypothesis of trend-stationarity. This suggests that the time series may not be trend-stationary. As discussed by Kumar (2023), if both tests conclude the the given series is non-stationary, there is a high indication thart the series is in fact non-stationary. Which is commonly the case for stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print out results in customised manner\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "def kpss_test(timeseries):\n",
    "    print ('Results of KPSS Test:')\n",
    "    kpsstest = kpss(timeseries, regression='c', nlags=\"auto\")\n",
    "    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','#Lags Used'])\n",
    "    for key,value in kpsstest[3].items():\n",
    "        kpss_output['Critical Value (%s)'%key] = value\n",
    "    print (kpss_output)\n",
    "\n",
    "# Call the function and run the test\n",
    "\n",
    "kpss_test(new_df['I-CIP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc1b50",
   "metadata": {},
   "source": [
    "Since both the ADF and KPSS tests indicate that the I-CIP series is non-stationary:\n",
    "\n",
    "- The ADF test failed to reject the null hypothesis of non-stationarity (high p-value 0.99).\n",
    "- The KPSS test rejected the null hypothesis of stationarity (low p-value 0.01).\n",
    "\n",
    "\n",
    "In an ideal world i would go back and test the data with the holidays and add more years back to further the historical analysis. At this stage, with the time left to compolete the project this methods above will be kept in a drawer for another moment.\n",
    "\n",
    "- Standardization/Normalization: Use techniques like z-score standardization or min-max scaling to scale the coffee prices to a common range. This helps to handle features with different scales. It removes the mean and scales the data to have unit variance, which can make it easier for some models to learn the patterns in the data.\n",
    "\n",
    "- Differencing: Compute differences between consecutive stock prices to remove trends and make the data stationary, if necessary, which is often required for time series forecasting or machine learning tasks.\n",
    "\n",
    "Source: Binita Kumari and Tripti Swarnkar \n",
    "    International Journal of Advanced Technology and Engineering Exploration, Vol 8(83)1339 \n",
    "    \n",
    "Apply time series analysis techniques beyond autocorrelation and decomposition, as they will be done in further section, and investigate moving averages,to identify further seasonal components, trends, and other patterns in the data. This could also be used as additional features for modeling\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def21fb0",
   "metadata": {},
   "source": [
    "### ADF/KPPS tests for all categories:\n",
    "\n",
    "First cvode bvelow will renew the trend visualisation across categories\n",
    "\n",
    "The second code performs the adf and kpps tests on all categories to confirm a similar pattern as observed on previous section of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e43b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot each category to visually inspect trends\n",
    "categories = ['I-CIP', 'colombian_milds', 'other_milds', 'brazilian_nat', 'robustas']\n",
    "\n",
    "for category in categories:\n",
    "    new_df[category].plot(title=f'Time Series of {category}', figsize=(10, 5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f2181",
   "metadata": {},
   "source": [
    "The idea behind the unit test is that it finds out how\n",
    "strongly a time series is determined by a trend.\n",
    "\n",
    "- Null Hypothesis (H0): The time series can be represented by a unit root that is not stationary.\n",
    "- Alternative Hypothesis (H1): The time series is stationary.\n",
    "\n",
    "<br>\n",
    "Interpretation of the p-value\n",
    "\n",
    "- P-value > 0.05: Accepts the Null Hypothesis\n",
    "- P-value < 0.05: Rejects the Null Hypothesis\n",
    "\n",
    "P. Yamak et al (2019) says it's common to use both tests to test fpr stationarity, and in this case, the ADF test provides more convincing evidence of stationarity compared to the KPSS test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    print(f\"--- Stationarity Test for {category} ---\")\n",
    "    # ADF Test\n",
    "    adf_result = adfuller(new_df[category].dropna())\n",
    "    print('ADF Statistic:', adf_result[0])\n",
    "    print('p-value:', adf_result[1])\n",
    "\n",
    "    # KPSS Test\n",
    "    kpss_result = kpss(new_df[category].dropna(), regression='c')\n",
    "    print('KPSS Statistic:', kpss_result[0])\n",
    "    print('p-value:', kpss_result[1])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f3dca",
   "metadata": {},
   "source": [
    "## 2.8 Decomposing data for Seasonality and Trend\n",
    "\n",
    "\n",
    "\"Predicting future values is easier with a stationary series, enhancing the precision of statistical models. Thus, for effective predictions, ensuring stationarity is crucial.\" https://www.analyticsvidhya.com/blog/2018/09/an-introduction-to-non-stationary-time-series-in-python/\n",
    "\n",
    "The first part of the next code will consists of a lineplot to visualize the time series data of how the values change over time. Next is performed a seasonal decomposition for each column in the DataFrame. It iterates over each column, decomposes the time series into trend, seasonal, and residual/noise components using the seasonal decompose function from statsmodels, and plots the decomposed components. This helps in understanding the underlying patterns and trends in the data.\n",
    "\n",
    "\"Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting.\" Brownlee,2020 https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/\n",
    "\n",
    "\"Decomposition is the process of breaking down a time series into its constituent components, typically trend, seasonality, and residual (noise). Trend represents the long-term movement or directionality of the data, seasonality captures periodic fluctuations, and residual represents random fluctuations or noise. Decomposition helps understanding the underlying patterns in the data before modeling and forecasting\" Dey, 2024 (https://medium.com/@roshmitadey/time-series-decomposition-62cbf31ab65e) Bonaros, 2021 (https://towardsdatascience.com/time-series-decomposition-in-python-8acac385a5b2)\n",
    "\n",
    "Finally, the last part of the code returns the autocorrelation function (ACF) and partial autocorrelation function (PACF) plots for each column in the DataFrame. Each plot gives us insights about the autocorrelation and partial autocorrelation at different lags, to help identify the order of autoregressive (AR) and moving average (MA) terms for time series modeling.\n",
    "\n",
    "Other ways to decompose data for timeseries models, (advanced tecniques for more complex, irregular or non-stationary data)\n",
    "\n",
    "- Seasonal and Trend decomposition using Loess (STL)\n",
    "- Wavelet Decomposition\n",
    "- Singular Spectrum Analysis (SSA)\n",
    "(Dey, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2267ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seasonal decompositions with different periods.\n",
    "\n",
    "periods = [63, 21, 5]  # Quartely, Monthly and Weekly (considering business days)\n",
    "# Function to generate the plots for all periods.\n",
    "for period in periods:\n",
    "    decompositions = seasonal_decompose(new_df['I-CIP'], model='additive', period=period)\n",
    "\n",
    "    # Plotting the components of the decomposition\n",
    "    plt.rcParams.update({'figure.figsize': (8,8)})\n",
    "    print(f\"Seasonal Decomposition with Period = {period}\")\n",
    "    decompositions.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb5963",
   "metadata": {},
   "source": [
    "From a quick inspection, we can see the a comparison between the raw timeseries data for ICIP prices over the 20m, as well as its fluctuations and an overall trend. \n",
    "\n",
    "**TREND** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f58fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initially had the seasonal deco and ACF of all columns gathered in the same code, \n",
    "#ended up being too crowded/poluted and hard to comprehend. They are marked as comments bellow. \n",
    "\n",
    "# Visualize how values change over time\n",
    "new_df['I-CIP'].plot(figsize=(12, 6))\n",
    "plt.title('Time Series Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "# Seasonal decomposition\n",
    "for category in categories:\n",
    "    result = seasonal_decompose(new_df[category], model='additive', period=21)  # Assuming a monthly pattern business days\n",
    "    result.plot()\n",
    "    plt.suptitle(f'Seasonal Decomposition of {category}')\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ACF and PACF plots\n",
    "for category in categories:\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 6))\n",
    "    plot_acf(new_df[category], ax=ax[0], lags=40)\n",
    "    plot_pacf(new_df[category], ax=ax[1], lags=40)\n",
    "    ax[0].set_title(f'ACF and PACF for {column}')\n",
    "    plt.tight_layout()  # adjust subplot parameters\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aae016",
   "metadata": {},
   "source": [
    "### Decomposition of I-CIP with a period = 5 (Freq as Business days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(new_df['I-CIP'], model='multiplicative', period=5) #weekly\n",
    "\n",
    "# Plot the original and decomposed components\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(new_df['I-CIP'], label='Original')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(result.trend, label='Trend')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(result.seasonal, label='Seasonal')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(result.resid, label='Residual')\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle( 'Decomposition of I-CIP values period = 5')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7b206",
   "metadata": {},
   "source": [
    "### Decomposition of I-CIP with a period of 21 (montly business days)\n",
    "\n",
    " from the seasonal section of graph, it  seems to be present a **regular fluctuation in coffee prices with peaks and troughs that repeat every 21 days** \n",
    " \n",
    " in terms of residuals,  if there are significant patterns in the residuals, it could indicate that some aspects of seasonality or trend have not been fully captured by the model. (residulas: the more random the \"better\" = more white noise\n",
    "\n",
    "\n",
    "\n",
    "\"Residual: These are the irregularities or 'noise' that cannot be attributed to the trend or seasonality. It represents the randomness or irregular occurrences in the data after removing the trend and seasonal components. Ideally, the residuals should not show any pattern (be white noise) if the model has captured all the systematic information in the data.\" https://towardsdatascience.com/time-series-decomposition-8f39432f78f9#:~:text=Compute%20the%20seasonal%20component%2C%20S,%2F(TR)%20for%20multiplicative%20model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(new_df['I-CIP'], model='multiplicative', period=21) #montly BD\n",
    "\n",
    "# Plot the original and decomposed components\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(new_df['I-CIP'], label='Original')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(result.trend, label='Trend')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(result.seasonal, label='Seasonal')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(result.resid, label='Residual')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.suptitle( 'Decomposition of I-CIP values period = 21')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(new_df['I-CIP'], model='multiplicative', period=63) # Quarterly\n",
    "\n",
    "# Plot the original and decomposed components\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(new_df['I-CIP'], label='Original')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(result.trend, label='Trend')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(result.seasonal, label='Seasonal')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(result.resid, label='Residual')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.suptitle( 'Decomposition of I-CIP values period = 21')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e71cce",
   "metadata": {},
   "source": [
    "## AUTOCORRELATION -  ACF PACF plots \n",
    "\n",
    "https://itsudit.medium.com/deciphering-acf-and-pacf-plots-a-guide-to-time-series-forecasting-3323948935fb#:~:text=To%20interpret%20ACF%20and%20PACF,a%20trend%20in%20the%20data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca63955b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "#new_df['I-CIP'].plot(figsize=(14, 7))\n",
    "#plt.title('Time Series Data')\n",
    "#plt.show()\n",
    "\n",
    "# Perform a seasonal decomposition to infer potential seasonality\n",
    "result = seasonal_decompose(new_df['I-CIP'], model='multiplicative', period=21)  #\n",
    "result.seasonal.plot(figsize=(14, 7))\n",
    "plt.title('Seasonal Decomposition - Seasonality Component')\n",
    "plt.show()\n",
    "\n",
    "# ACF and PACF plots\n",
    "plot_acf(new_df['I-CIP'].dropna(), lags=40)  # \n",
    "plt.title('Autocorrelation Function')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(new_df['I-CIP'].dropna(), lags=40, method='ywm')  # method='ywm' to avoid statistical issues\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750d657",
   "metadata": {},
   "source": [
    "###  autocorrelation\n",
    "\n",
    "- The distance between peaks or troughs can give you an indication of the length of the seasonal cycle.\n",
    "- The gradual decline from 1 at lag 0 indicates how the correlation between the values decreases as the lags increase\n",
    "- The significant negative autocorrelation at higher lags could be due to the ??? complex seasonal pattern? \n",
    "\n",
    "\n",
    "As a complement to the plot above to reiterate the lack of a defined trend, but the high inlfuence of seasonality. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1529122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "# Draw Plot\n",
    "plt.rcParams.update({'figure.figsize':(9,5), 'figure.dpi':120})\n",
    "autocorrelation_plot(new_df['I-CIP'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using daily data and interested in montly seasonality\n",
    "decomp = seasonal_decompose(new_df['I-CIP'], model='multiplicative', period=63)\n",
    "\n",
    "# Plotting the seasonal component\n",
    "decomp.seasonal.plot()\n",
    "plt.title('Seasonal Decomposition - Seasonality Component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91261e6",
   "metadata": {},
   "source": [
    "## Fourier Analysis (identify dominant cycles, if any):\n",
    "\n",
    "The Fourier transform can help remove noise by transforming time series data into the frequency domain, allowing  to filter out noise frequencies. Then, the Fourier inverse transform is applied to obtain the time series after filtering\n",
    "https://medium.com/@tubelwj/guide-to-time-series-data-pre-processing-methods-0a6df7ee054f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a Fourier transform\n",
    "fft_result = np.fft.rfft(new_df['I-CIP'])\n",
    "frequencies = np.fft.rfftfreq(len(new_df['I-CIP']), d=1)  # d is the sample spacing\n",
    "\n",
    "# Plotting the power spectrum\n",
    "power_spectrum = np.abs(fft_result)\n",
    "plt.plot(frequencies, power_spectrum)\n",
    "plt.title('Fourier Analysis - Power Spectrum')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f508908",
   "metadata": {},
   "source": [
    "From the power spectrum in the fourier analisis above, is observed a large spike near frequency = 0 indicates that most of the variance in the I-CIP data is at very low frequencies. This corresponds to long-term trends or very slow changes in the data.\n",
    "This could suggest that there is a strong trend or very low-frequency component in the data, which could represent overall market behavior rather than short-term cycles.\n",
    "\n",
    "Beyond the initial spike, the amplitudes drop significantly and remain low as the  frequencies increase. This suggests that there aren't any strong, regular high-frequency cycles (such as daily or weekly cycles) in the data, or that these patterns are subtle compared to the low-frequency trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6314c5",
   "metadata": {},
   "source": [
    "### Detrending I-CIP \n",
    "\n",
    "The plots above show us the trend components to I-cIp prices, and the weekly, quaterly fluctuations around the same periods.\n",
    "Below, as an attemot to isolating short term fluctuations, since this dataset is relatively short in terms of historical data,by isolating the trend component, is possible to highlight the daily seasonal variations. \n",
    "\n",
    "This detrended series allows for a clearer analysis of short-term volatility and cyclical patterns in the I-CIP prices, without the influence of overall market trends, making it useful for exploring potential seasonal behaviors and other short-term irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8b8c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using statmodels: Subtracting the Trend Component.\n",
    "\n",
    "# Ensure the index is set correctly as a datetime index\n",
    "#new_df.index = pd.to_datetime(new_df.index)\n",
    "\n",
    "# Now decompose with the known period\n",
    "result_mul = seasonal_decompose(new_df['I-CIP'], model='multiplicative', period=5, extrapolate_trend='freq')\n",
    "\n",
    "# Subtract the trend from the original data to detrend\n",
    "detrended = new_df['I-CIP'] - result_mul.trend\n",
    "\n",
    "# Now you can plot the detrended data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(detrended, label='Detrended Data')\n",
    "plt.title('I-CIP prices detrended by subtracting the trend component')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896633b7",
   "metadata": {},
   "source": [
    "### Deseasonalized prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f219d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform seasonal decomposition\n",
    "result_mul = seasonal_decompose(new_df['I-CIP'], model='multiplicative', period=5, extrapolate_trend='freq')\n",
    "\n",
    "# Deseasonalize\n",
    "deseasonalized = new_df['I-CIP'] / result_mul.seasonal\n",
    "\n",
    "# Plot the deseasonalized data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(deseasonalized, label='Deseasonalized Data')\n",
    "plt.title('I-CIP values Deseasonalized', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01230277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plot_acf(detrended.dropna(), lags=30)\n",
    "plt.title(\"Autocorrelation of Detrended Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test\n",
    "adf_result = adfuller(detrended.dropna())\n",
    "print('ADF Statistic:', adf_result[0])\n",
    "print('p-value:', adf_result[1])\n",
    "\n",
    "# KPSS Test\n",
    "kpss_result = kpss(detrended.dropna(), regression='c')\n",
    "print('KPSS Statistic:', kpss_result[0])\n",
    "print('p-value:', kpss_result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160f1b3",
   "metadata": {},
   "source": [
    "## 2.9 Comparing standartisation vs Normalisation vs Differentiation\n",
    "\n",
    "This section will focus on experimwenting different data preprocessing to:\n",
    "- obtain a detrended version of the dataset to find more optimal acf/pacf correlation between the values. \n",
    "- using data transformation can help to better visualise the seasonality when compared to the same lags. \n",
    "\n",
    "\n",
    " the decomposition suggests that yes, there is  a repeated pattern, which could be further investigated by analyzing the ACF with a higher number of lags. This could help to confirm whether the seasonal pattern seen in the decomposition plot also appears as periodic spikes in the ACF plot, which is what would be typically expect for seasonal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7203485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataframe of only icip prices  to a new variable\n",
    "df = new_df['I-CIP']\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e53903",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization using the mean value\n",
    "mean_price = df['I-CIP'].mean()\n",
    "std_price = df['I-CIP'].std()\n",
    "df['price_standardized'] = (df['I-CIP'] - mean_price) / std_price\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e3947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization using min/max scale\n",
    "min_price = df['I-CIP'].min()\n",
    "max_price = df['I-CIP'].max()\n",
    "df['price_normalized'] = (df['I-CIP'] - min_price) / (max_price - min_price)\n",
    "\n",
    "\n",
    "#Min-max normalisation has one notable drawback: itstruggles to deal with outliers\n",
    "\n",
    "#Source: Binita Kumari and Tripti Swarnkar \n",
    "    ##International Journal of Advanced Technology and Engineering Exploration, Vol 8(83)1339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b305b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f76e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing\n",
    "df['price_diff'] = df['I-CIP'].diff()\n",
    "\n",
    "# Dropping missing values (first rows become nan by defautl)\n",
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87659d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a655b",
   "metadata": {},
   "source": [
    "## a. Visualising the Standardization and Normalization aplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d692a57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histogram for original data\n",
    "fig_hist_original = px.histogram(df, x='I-CIP', title='Histogram: Original Data')\n",
    "fig_hist_original.show()\n",
    "\n",
    "# Plot histogram for standardized data\n",
    "fig_hist_standardized = px.histogram(df, x='price_standardized', title='Histogram: Standardized Data')\n",
    "fig_hist_standardized.show()\n",
    "\n",
    "# Plot histogram for normalized data\n",
    "fig_hist_normalized = px.histogram(df, x='price_normalized', title='Histogram: Normalized Data')\n",
    "fig_hist_normalized.show()\n",
    "\n",
    "# Plot histogram for diff data\n",
    "fig_hist_normalized = px.histogram(df, x='price_diff', title='Histogram: Diff Data')\n",
    "fig_hist_normalized.show()\n",
    "\n",
    "# Plot line plot for original data vs differenced data\n",
    "fig_line = go.Figure()\n",
    "fig_line.add_trace(go.Scatter(x=df.index, y=df['I-CIP'], mode='lines', name='Original Data'))\n",
    "fig_line.add_trace(go.Scatter(x=df.index, y=df['price_diff'], mode='lines', name='Differenced Data'))\n",
    "fig_line.update_layout(title='Line Plot: Original Data vs Differenced Data', xaxis_title='Date', yaxis_title='Price')\n",
    "fig_line.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5881f3a4",
   "metadata": {},
   "source": [
    "The first histograms with standardised and normalised values only set the new scales for the prices, but the distribution and overall statistical nature of the data remains the same as the original. However in the diff prices, is clear to see how they change the distribution to a normal dist and reduces the standard deviation from 11 to 2. Below we can also compare the values to see how they behave before and after the first diff. However, to be sure that one diff should be sufficient before moving to modeling, we can check the adf and kpss once again to see the stationarity level. \n",
    "\n",
    "-  Differencing removes the trend component from the data, making it stationary. Real price values may exhibit trends over time, which can make modeling more challenging. These values represent the the changes or fluctuations in the variable from one time point to the next.\n",
    "\n",
    "\n",
    "<i>\" Differenced values are often used in conjunction with autoregressive integrated moving average (ARIMA) models, which are well-suited for stationary time series. Forecasting: Differenced values can lead to forecasts of the changes in the variable rather than the actual values themselves. Forecasts based on real price values predict future levels of the variable directly.\"<i/> https://medium.com/towards-data-science/forecasting-time-series-data-stock-price-analysis-324bcc520af5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8726cb5",
   "metadata": {},
   "source": [
    "### b. ADF/KPPS tests for diff values - checking stationarity again\n",
    "\n",
    "ADF eith pvalue quite small (4.99e-11) nearly zero which is below the statistical significane and therefore we reject the null hypotesis for non-stationarity. \n",
    "\n",
    "KPPS p value 0.1 (>0.05)\n",
    "Both the ADF and KPSS tests support that the differentiated series is stationary. whihc is also complemented by the barplots above showing the new distribution of diff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test\n",
    "result = adfuller(df['price_diff'], autolag='AIC')\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "for key, value in result[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')\n",
    "\n",
    "# KPSS Test\n",
    "result = kpss(df['price_diff'], regression='c')\n",
    "print('\\nKPSS Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "for key, value in result[3].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d51ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4837d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cbb59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d73ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e45f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05dbcd15",
   "metadata": {},
   "source": [
    "## 3. Data Preparation for Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3585662",
   "metadata": {},
   "source": [
    "# Regression Line>> Finding the trend of the original prices\n",
    "\n",
    "The trend slope in regression tasks is obtainned by taking the **difference between two data points and dividing it by the amount of time between them.**\n",
    "\n",
    "\n",
    "a positive trend indicates the future prices tend to increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ed1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Extract features (dates) and target variable (prices)\n",
    "X = new_df.index.to_julian_date().values.reshape(-1, 1)\n",
    "y = new_df['I-CIP'].values\n",
    "\n",
    "# Fit linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X, y)\n",
    "\n",
    "# Get the slope (trend) of the regression line\n",
    "trend = lr_model.coef_[0]\n",
    "print(\"Trend (slope) of the regression line:\", trend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99af280",
   "metadata": {},
   "source": [
    "#### Visualising the trend line\n",
    "\n",
    "The positive trend of 0.1413 in your regression line indicates that, on average, the I-CIP price increases by 0.1413 units per time step, over time, there is a steady upward movement in prices, reflecting a long-term increasing trend in the I-CIP values.\n",
    "\n",
    "This also can inidcate models like SARIMA and LSTM could be more apropriate choices as they include a trend componetnet in their structure, as the data exhibits a clear upward movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the original data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(new_df.index, new_df['I-CIP'], label='Original Data')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(new_df.index, lr_model.predict(X), color='red', linestyle='--', label='Regression Line')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('I-CIP Price')\n",
    "plt.title('Trend Analysis with Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81af0c9",
   "metadata": {},
   "source": [
    "### How about using my detrended dataframe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (dates) and target variable (prices)\n",
    "X = detrended.index.to_julian_date().values.reshape(-1, 1)\n",
    "y = detrended.values\n",
    "\n",
    "# Fit linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X, y)\n",
    "\n",
    "# Get the slope (trend) of the regression line\n",
    "trend = lr_model.coef_[0]\n",
    "print(\"Trend (slope) of the regression line:\", trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the original data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(detrended.index, detrended, label='Detrended Data')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(detrended.index, lr_model.predict(X), color='red', linestyle='--', label='Regression Line')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('I-CIP Price')\n",
    "plt.title('Trend Analysis with Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1726b86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cade5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19555828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da45b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54317a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75135e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11677e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa2921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1c497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259bb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01636ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7697e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52ca70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef58c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "757d5306",
   "metadata": {},
   "source": [
    "## 3.1 Split train/test\n",
    "Checking for optimal ratio between training and testing \n",
    "\n",
    "This  time series data has observations recorded only on weekdays (Monday to Friday), so it's important to account for this when splitting the data for training and testing following a sequential splitting (also the data needs to be sorted chronologically by the timestamps). To ensure a proper split for time series data, data scientists typically create a training set to contain observations from the earlier dates and the testing set to contain observations from the later dates. \n",
    "\n",
    "\n",
    "- for this first attemp the origibal  values will be used for modeling.and a second attempt with diff values to see if there is any different outcomes. \n",
    "\n",
    "While real price values capture the actual levels of the variable of interest, differenced values are often preferred for time series analysis when stationarity is desired and when modeling changes or fluctuations in the data. (Thakar, 2020)\n",
    "\n",
    "On further sections data imputation techniques are planned to be used to fill all dates and have a more complete dataset. using moving average and foward fill techniques. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the dates range before splitting\n",
    "dates_array = df.index\n",
    "\n",
    "# Print the array of dates\n",
    "print(\"Array of Dates (Index):\")\n",
    "print(dates_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5fe5b7",
   "metadata": {},
   "source": [
    "**double check fo any missing dates in the index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a date range for 366 days from the start of your data\n",
    "# Adjust the period accordingly if you have data spanning multiple years or a different time frame\n",
    "start_date = df.index.min()\n",
    "end_date = df.index.max()  \n",
    "\n",
    "\n",
    "# Generate a range of business days within this period\n",
    "business_days = pd.bdate_range(start=start_date, end=end_date)\n",
    "\n",
    "# Now compare the business_days with your DataFrame's index to find out missing dates\n",
    "missing_dates = business_days.difference(df.index)\n",
    "\n",
    "print(f\"Total number of expected business days: {len(business_days)}\")\n",
    "print(f\"Total number of actual days in data: {df.shape[0]}\")\n",
    "print(f\"Total number of missing dates: {len(missing_dates)}\")\n",
    "print(\"Missing dates are:\")\n",
    "print(missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14614d6",
   "metadata": {},
   "source": [
    "## Splitting into train/test sets \n",
    "\n",
    "### Following the 70/15/15 ratio\n",
    "\n",
    "70% for training\n",
    "15% for validation\n",
    "15% for testing\n",
    "\n",
    "considering a total of 433 observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f0bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes\n",
    "train_size = int(len(df) * 0.7)       # 70% for training\n",
    "val_size = int(len(df) * 0.15)        # 15% for validation\n",
    "test_size = len(df) - train_size - val_size  # Remaining 15% for testing\n",
    "\n",
    "# Split the data\n",
    "train = df.iloc[:train_size]                      # First 303 observations = 70%\n",
    "validation = df.iloc[train_size:train_size + val_size]  # Next 64 observations = 15%\n",
    "test = df.iloc[train_size + val_size:]            # Last 66 observations = 15% aprx\n",
    "\n",
    "# Print the sizes to confirm\n",
    "print(\"Training Set Size:\", len(train))         \n",
    "print(\"Validation Set Size:\", len(validation))  \n",
    "print(\"Testing Set Size:\", len(test))           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a08da3",
   "metadata": {},
   "source": [
    "Convert the index to a numeric format for use in the regression model.>>> regression models can’t work with datetime indices directly, we convert the indices to numeric values (integer representations of the timestamps) for X_train, X_val, and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index to numerical representation for regression models\n",
    "X_train = pd.to_numeric(train.index).values.reshape(-1, 1)\n",
    "y_train = train['I-CIP']\n",
    "\n",
    "X_val = pd.to_numeric(validation.index).values.reshape(-1, 1)\n",
    "y_val = validation['I-CIP']\n",
    "\n",
    "X_test = pd.to_numeric(test.index).values.reshape(-1, 1)\n",
    "y_test = test['I-CIP']\n",
    "\n",
    "# Print the sizes of each set to verify\n",
    "print(\"Training Set Size:\", len(train))\n",
    "print(\"Validation Set Size:\", len(validation))\n",
    "print(\"Testing Set Size:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfdcf77",
   "metadata": {},
   "source": [
    "#### Verify Data Dimensions:\n",
    "Make sure that X_test and y_test have the same number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07928a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"y_val shape:{y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec129b",
   "metadata": {},
   "source": [
    "#####  Attempt to create a storage for all metrics from different models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c29fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the main dictionary to store metrics for all models\n",
    "all_metrics = {}\n",
    "\n",
    "# Define a function to calculate and store metrics\n",
    "def store_metrics(model_name, y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    # Create a dictionary for the current model's metrics\n",
    "    error_metrics = {\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    # Add the current model's metrics to the main dictionary\n",
    "    all_metrics[model_name] = error_metrics\n",
    "\n",
    "\n",
    "\n",
    "# Print all metrics\n",
    "for model, metrics in all_metrics.items():\n",
    "    print(f\"Metrics for {model}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easier comparison IN FUTURE SECTIONS? \n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics).T  # Transpose for better readability\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706012c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79f07ea",
   "metadata": {},
   "source": [
    "# 4. Regression Models \n",
    "\n",
    "\n",
    "### Machine Learning Baseline Models\n",
    "\n",
    "For this section \n",
    "\n",
    "Characteristcs of the data observed thus far can justify the selection of auto-regressive models such as SARIMA(X) and LSTM. However 3 baseline models can be aplied first to create a benchmark and increase the interpretstion of results,.\n",
    "\n",
    "**Linear Regression:** The simplest model that assumes a linear relationship between the input variables (x) and the single output variable (y). It finds the line (in two dimensions) that best fits the data points. It's easy to understand and interpret <mark> but can be limited in handling complex data patterns without transformations or additions like polynomial features. <mark/> doesnt consider fluctuations and seasonality\n",
    "    \n",
    "    \n",
    "**Decision Tree Regressor** This model uses a decision tree to go from observations about an item  (branches) to conclusions about the item's target value (leaves). It's a simple and interpretable model (can be susceptible to overfitting).\n",
    "\n",
    "**Random Forest Regressor:** An combination of  methods that fits a number of decision tree regressors on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. It's more robust and accurate than a single decision tree.\n",
    "\n",
    "\n",
    "**Gradient Boosting Regressor:** An ensemble technique that builds models sequentially, each new model correcting errors made by previous models. It combines weak predictive models to create a strong predictor. Gradient boosting can be used for both regression and classification tasks and is known for its effectiveness, especially with non-linear data.\n",
    "\n",
    "\n",
    "**SVR (Support Vector Regression):**  uses the principles of support vector machines for regression tasks. It tries to fit the error within a certain threshold and is robust to outliers. <mark> SVR can be used for both linear and non-linear data.<mark/> \n",
    "\n",
    "Sources: \n",
    "    \n",
    "[https://towardsdatascience.com/a-beginners-guide-to-regression-analysis-in-machine-learning-8a828b491bbf\n",
    "https://otexts.com/fpp2/regression.html]\n",
    "    \n",
    "\n",
    "    \n",
    "### 4.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806f582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb889879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb469f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = model.predict(X_val)\n",
    "\n",
    "# Calculate and print evaluation metrics for the validation set\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "val_mae = mean_absolute_error(y_val, val_predictions)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "val_mape = np.mean(np.abs((y_val - val_predictions) / y_val)) * 100\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", val_mse)\n",
    "print(\"Mean Absolute Error (MAE):\", val_mae)\n",
    "print(\"Root Mean Squared Error (RMSE):\", val_rmse)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", val_mape, \"%\")\n",
    "\n",
    "# After evaluating on the validation set, make predictions on the testing set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate and print evaluation metrics for the testing set\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mape = np.mean(np.abs((y_test - test_predictions) / y_test)) * 100\n",
    "\n",
    "print(\"\\nTesting Set Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", test_mse)\n",
    "print(\"Mean Absolute Error (MAE):\", test_mae)\n",
    "print(\"Root Mean Squared Error (RMSE):\", test_rmse)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", test_mape, \"%\")\n",
    "\n",
    "# Plot actual vs. predicted values for both validation and testing sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(validation.index, y_val, label='Validation Actual')\n",
    "plt.plot(validation.index, val_predictions, label='Validation Predicted')\n",
    "plt.plot(test.index, y_test, label='Testing Actual')\n",
    "plt.plot(test.index, test_predictions, label='Testing Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('I-CIP')\n",
    "plt.title('Actual vs. Predicted Values for Validation and Testing Sets')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df14eb",
   "metadata": {},
   "source": [
    "linear regression= not the most appropriate since  the outcomes will not capture the true trend in the prices.\n",
    "The high error metrics ((MSE): 207, (RMSE): 45.550 and (MAPE): 20.00) ilustrate how different the predictions are from the original values. the validation/ttesting sets are with a very flat line showing the model is not learning the patterns in data. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for Linear Regression\n",
    "model_name = \"Linear Regression\"\n",
    "# After fitting the model, get predictions\n",
    "predictions_lr = lr_model.predict(X_test)\n",
    "store_metrics(model_name, y_test, predictions_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b441315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b5715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4341c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b71a68",
   "metadata": {},
   "source": [
    "## 4.2 Random Forrest \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d661e",
   "metadata": {},
   "source": [
    "### 1. Feature engineering for Random Forest\n",
    "\n",
    "Adding new time-based features like **day of the week, week of the year, month, and quarter** to the dataset, that can help the model capture any potential seasonality or cyclical patterns in the data and is a step further from the linear regression. This will hopefully decrease the error measures. Lazzeri (2021) [https://medium.com/data-science-at-microsoft/introduction-to-feature-engineering-for-time-series-forecasting-620aa55fcab0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fb483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding day of the week, week of the year, month, and quarter to df\n",
    "df['DayOfWeek'] = df.index.dayofweek\n",
    "df['WeekOfYear'] = df.index.isocalendar().week  # Use isocalendar().week to get the week number\n",
    "df['Month'] = df.index.month\n",
    "df['Quarter'] = df.index.quarter\n",
    "\n",
    "# Define features and target variable\n",
    "features = ['DayOfWeek', 'WeekOfYear', 'Month', 'Quarter']\n",
    "X = df[features]\n",
    "y = df['I-CIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c2b50",
   "metadata": {},
   "source": [
    "### 2. Data split 70/15/15\n",
    "\n",
    "Splitting data nto training, validation, and testing sets using an 70/15/15 ratio. \n",
    "\n",
    "The validation set allows to assess model performance before final testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split off the test set (15%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42) #42 as default\n",
    "\n",
    "# Then split the remaining 85% into training (70%) and validation (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)  # 15% of 85% is ~15%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a1941",
   "metadata": {},
   "source": [
    "### 3. Train Model _ Random Forest REgressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66433c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49243b2c",
   "metadata": {},
   "source": [
    "### 4. Evaluate Validation Set\n",
    "\n",
    "still shows a high error for the model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "val_mae = mean_absolute_error(y_val, val_predictions)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "val_mape = np.mean(np.abs((y_val - val_predictions) / y_val)) * 100\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", val_mse)\n",
    "print(\"Mean Absolute Error (MAE):\", val_mae)\n",
    "print(\"Root Mean Squared Error (RMSE):\", val_rmse)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", val_mape, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56335d0b",
   "metadata": {},
   "source": [
    "### 5.Evaluation of Testing set \n",
    "\n",
    "### Actual vs. Predicted Values\n",
    "\n",
    "error metrics are already smaller than LR however the predicted values are quite far from the actual values, this means there is room for improvement in this model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c57790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mape = np.mean(np.abs((y_test - test_predictions) / y_test)) * 100\n",
    "\n",
    "print(\"\\nTesting Set Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", test_mse)\n",
    "print(\"Mean Absolute Error (MAE):\", test_mae)\n",
    "print(\"Root Mean Squared Error (RMSE):\", test_rmse)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", test_mape, \"%\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, test_predictions, color='blue', alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs. Predicted Values (Random Forest)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621972d6",
   "metadata": {},
   "source": [
    "### 6. Improving the model\n",
    "\n",
    "possible further steps for improve the RF model:\n",
    "- using GridSearchCv for a Hyperparameter Tuning for Random Forrest\n",
    "- trying modeling with data in a different scale (using standardised prices)\n",
    "- add lag features (??) \n",
    "- use a different approach for cross validation with a TIMESERIES SPLIT from sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, val_index in tscv.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    val_predictions = model.predict(X_val)\n",
    "    print(\"Validation MSE:\", mean_squared_error(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082a792",
   "metadata": {},
   "source": [
    "### 7.  Random Forest Model Using price_standardized with Time Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = ['DayOfWeek', 'WeekOfYear', 'Month', 'Quarter']\n",
    "X = df[features]\n",
    "y = df['price_standardized']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "predictions = rf_model.predict(X_test)\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "\n",
    "\n",
    "# Print error metrics\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape, \"%\")\n",
    "\n",
    "# Store error metrics in a dictionary for later comparison\n",
    "error_metrics = {\n",
    "    'Model': 'Random Forest',\n",
    "    'MSE': mse,\n",
    "    'MAE': mae,\n",
    "    'RMSE': rmse,\n",
    "    'MAPE': mape\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fca08",
   "metadata": {},
   "source": [
    "\n",
    "Visualize Actual vs. Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c92965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, predictions, color='blue', alpha=0.5, label='Predicted vs Actual')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Ideal Fit')\n",
    "plt.xlabel('Actual Price Standardized')\n",
    "plt.ylabel('Predicted Price Standardized')\n",
    "plt.title('Actual vs. Predicted Values (Random Forest)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbcaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffc64a81",
   "metadata": {},
   "source": [
    "### 8. Hyperparameter tuning for Random Forest\n",
    "\n",
    "\n",
    "Best Parameters:\n",
    "- 'max_depth': None, \n",
    "- 'min_samples_leaf': 4, \n",
    "- 'min_samples_split': 10, \n",
    "- 'n_estimators': 300}\n",
    "- Best Score: -0.0034037662152510474\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bde251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters grid for RandomForestRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    #'max_features': ['auto', 'sqrt', 'log2'] was showing excessive warnings? feature discontinued from sklearn!\n",
    "}\n",
    "\n",
    "# Initialize RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Perform Grid Search Cross-Validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)  # Assuming X_train and y_train are your training features and target\n",
    "\n",
    "# Get the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters from grid search\n",
    "best_params = {\n",
    "    'max_depth': None,\n",
    "    'max_features': 'auto',\n",
    "    'min_samples_leaf': 4,\n",
    "    'min_samples_split': 10,\n",
    "    'n_estimators': 300\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestRegressor with the best parameters\n",
    "regressor = RandomForestRegressor(**best_params)\n",
    "\n",
    "# Fit the model to your data\n",
    "# X_train and y_train should be your training data\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Now the model is ready to make predictions or be evaluated further\n",
    "predictions = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot of actual vs predicted values\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Plot actual test data as points only (no lines)\n",
    "plt.plot(y_test.index, y_test, label='Actual Test Data', color='orange', marker='o', linestyle='', markersize=6)\n",
    "\n",
    "# Plot predictions as individual points with a different style\n",
    "plt.plot(y_test.index, predictions, label='Random Forest Predictions', color='blue', marker='x', linestyle='', markersize=6)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Random Forest Predictions vs Actual Test Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Target Variable')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "    regressor, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "train_scores_mean = -train_scores.mean(axis=1)\n",
    "validation_scores_mean = -validation_scores.mean(axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2900b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a62db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6810d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7f501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8767fb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44776b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4420c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef2560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7428d3c",
   "metadata": {},
   "source": [
    "## 4.3 SARIMA\n",
    "\n",
    "SARIMA and LSTM perfom on a separate notebook to avoid misplacing the variables and have the notebook better structured.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d49ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0f056",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define SARIMA model parameters\n",
    "\n",
    "p = 1  # Non-seasonal AR order\n",
    "d = 1  # Non-seasonal differencing\n",
    "q = 1  # Non-seasonal MA order\n",
    "P = 1  # Seasonal AR order\n",
    "D = 1  # Seasonal differencing\n",
    "Q = 1  # Seasonal MA order\n",
    "m = 21  # Seasonal period ( for monthly seasonality)\n",
    "\n",
    "# Initialize the model\n",
    "sarima_model = SARIMAX(train['I-CIP'],\n",
    "                       order=(p, d, q),\n",
    "                       seasonal_order=(P, D, Q, m),\n",
    "                       enforce_stationarity=False,\n",
    "                       enforce_invertibility=False)\n",
    "\n",
    "# Fit the model\n",
    "sarima_result = sarima_model.fit()\n",
    "\n",
    "# Summary of the model\n",
    "print(sarima_result.summary())\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = sarima_result.get_forecast(steps=len(test))\n",
    "predicted_means = predictions.predicted_mean\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train, label='Training Data')\n",
    "plt.plot(test, label='Actual Test Data')\n",
    "plt.plot(test.index, predicted_means, label='SARIMA Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(test, predicted_means)\n",
    "mae = mean_absolute_error(test, predicted_means)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print the error metrics\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411430b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5bdba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameters (modify p, d, q, P, D, Q as needed)\n",
    "p, d, q = 1, 1, 1\n",
    "P, D, Q, m = 1, 1, 1, 12  # Assuming monthly seasonality\n",
    "\n",
    "# Select the target column 'I-CIP')\n",
    "target_column = ['price_diff']\n",
    "train_target = train[target_column]\n",
    "test_target = test['price_diff']\n",
    "\n",
    "# Initialize the SARIMAX model\n",
    "sarima_model = SARIMAX(train_target,\n",
    "                       order=(p, d, q),\n",
    "                       seasonal_order=(P, D, Q, m),\n",
    "                       enforce_stationarity=False,\n",
    "                       enforce_invertibility=False)\n",
    "\n",
    "# Fit the model\n",
    "sarima_result = sarima_model.fit()\n",
    "\n",
    "# Summary of the model\n",
    "print(sarima_result.summary())\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = sarima_result.get_forecast(steps=len(test_target))\n",
    "predicted_means = predictions.predicted_mean\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train, label='Training Data')\n",
    "plt.plot(test_target, label='Actual Test Data')\n",
    "plt.plot(test_target.index, predicted_means, label='SARIMA Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(test_target, predicted_means)\n",
    "mae = mean_absolute_error(test_target, predicted_means)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print the error metrics\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e5015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696e48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42d3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56e7ad5d",
   "metadata": {},
   "source": [
    "\n",
    "Regression Tree, SVM and Linear\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
